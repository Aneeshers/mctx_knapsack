<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Expert Iteration for 3D Bin Packing</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <style>
        @font-face {
            font-family: 'Berkeley Mono';
            src: url('BerkeleyMonoTrial-Regular.otf') format('opentype');
            font-weight: normal;
            font-style: normal;
        }
        
         :root {
            --bg: #ffffff;
            --fg: #1a1a1a;
            --accent: #2563eb;
            --accent-light: #dbeafe;
            --code-bg: #f8f9fa;
            --border: #e5e7eb;
            --muted: #6b7280;
            --success: #059669;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Berkeley Mono', 'SF Mono', 'Consolas', monospace;
            background: var(--bg);
            color: var(--fg);
            line-height: 1.75;
            font-size: 14px;
        }
        
        .container {
            max-width: 860px;
            margin: 0 auto;
            padding: 60px 24px;
        }
        
        header {
            margin-bottom: 48px;
            border-bottom: 1px solid var(--border);
            padding-bottom: 32px;
        }
        
        .tag {
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: var(--accent);
            margin-bottom: 16px;
        }
        
        h1 {
            font-size: 28px;
            font-weight: 600;
            line-height: 1.3;
            margin-bottom: 20px;
            letter-spacing: -0.5px;
        }
        
        .subtitle {
            font-size: 15px;
            color: var(--muted);
            margin-bottom: 24px;
            line-height: 1.6;
        }
        
        .meta {
            font-size: 12px;
            color: var(--muted);
            margin-bottom: 24px;
        }
        
        .meta a {
            color: var(--fg);
            text-decoration: none;
        }
        
        .meta a:hover {
            color: var(--accent);
        }
        
        .buttons {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
        }
        
        .btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 10px 16px;
            font-family: inherit;
            font-size: 12px;
            text-decoration: none;
            border: 1px solid var(--border);
            background: white;
            color: var(--fg);
            transition: all 0.15s ease;
        }
        
        .btn:hover {
            border-color: var(--accent);
            color: var(--accent);
        }
        
        .btn-primary {
            background: var(--fg);
            color: white;
            border-color: var(--fg);
        }
        
        .btn-primary:hover {
            background: var(--accent);
            border-color: var(--accent);
            color: white;
        }
        
        .visual-abstract {
            margin: 48px 0;
            text-align: center;
        }
        
        .visual-abstract img {
            max-width: 100%;
            border: 1px solid var(--border);
        }
        
        .visual-abstract figcaption {
            font-size: 11px;
            color: var(--muted);
            margin-top: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .tldr {
            background: var(--code-bg);
            padding: 20px 24px;
            margin: 32px 0;
            border: 1px solid var(--border);
        }
        
        .tldr-label {
            font-size: 10px;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: var(--accent);
            margin-bottom: 8px;
            font-weight: 600;
        }
        
        .tldr p {
            margin: 0;
            font-size: 13px;
        }
        
        .insight-box {
            border: 2px solid var(--accent);
            padding: 24px;
            margin: 40px 0;
            background: var(--accent-light);
            font-size: 13px;
        }
        
        .insight-box pre {
            background: white;
            border: 1px solid var(--border);
            padding: 16px;
            margin: 16px 0 0 0;
            font-size: 11px;
            line-height: 1.5;
            white-space: pre;
            overflow-x: auto;
            color: var(--fg);
        }
        
        h2 {
            font-size: 18px;
            font-weight: 600;
            margin: 48px 0 20px 0;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border);
        }
        
        h3 {
            font-size: 14px;
            font-weight: 600;
            margin: 32px 0 12px 0;
            color: var(--accent);
        }
        
        p {
            margin-bottom: 16px;
        }
        
        pre {
            background: #1a1a1a;
            color: #e0e0e0;
            padding: 20px;
            overflow-x: auto;
            margin: 24px 0;
            font-size: 12px;
            line-height: 1.6;
            border-radius: 0;
        }
        
        code {
            font-family: 'Berkeley Mono', monospace;
        }
        
        .inline-code {
            background: var(--code-bg);
            padding: 2px 6px;
            font-size: 12px;
            border: 1px solid var(--border);
        }
        
        .keyword {
            color: #569cd6;
        }
        
        .function {
            color: #dcdcaa;
        }
        
        .string {
            color: #ce9178;
        }
        
        .comment {
            color: #6a9955;
        }
        
        .number {
            color: #b5cea8;
        }
        
        .decorator {
            color: #c586c0;
        }
        
        figure {
            margin: 40px 0;
        }
        
        figure img {
            width: 100%;
            border: 1px solid var(--border);
            background: white;
        }
        
        figcaption {
            font-size: 12px;
            color: var(--muted);
            margin-top: 12px;
            padding: 0 8px;
            line-height: 1.6;
        }
        
        figcaption strong {
            color: var(--fg);
        }
        
        .math-block {
            background: var(--code-bg);
            padding: 20px;
            margin: 24px 0;
            text-align: center;
            border: 1px solid var(--border);
            overflow-x: auto;
        }
        
        ul,
        ol {
            margin: 16px 0 16px 24px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            font-size: 12px;
        }
        
        th,
        td {
            border: 1px solid var(--border);
            padding: 10px 12px;
            text-align: left;
        }
        
        th {
            background: var(--code-bg);
            font-weight: 600;
        }
        
        .highlight-box {
            border-left: 3px solid var(--accent);
            padding: 16px 20px;
            margin: 32px 0;
            background: var(--code-bg);
        }
        
        .highlight-box h4 {
            color: var(--accent);
            margin-bottom: 8px;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 32px 0;
        }
        
        .comparison-item {
            border: 1px solid var(--border);
            padding: 20px;
            background: white;
        }
        
        .comparison-item h4 {
            font-size: 13px;
            margin-bottom: 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border);
        }
        
        .comparison-item.winner {
            border-color: var(--success);
            background: #ecfdf5;
        }
        
        .comparison-item.winner h4 {
            color: var(--success);
        }
        
        @media (max-width: 600px) {
            .comparison {
                grid-template-columns: 1fr;
            }
        }
        
        .findings {
            display: grid;
            gap: 16px;
            margin: 24px 0;
        }
        
        .finding {
            border-left: 3px solid var(--accent);
            padding: 12px 16px;
            background: var(--code-bg);
        }
        
        .finding-num {
            font-size: 10px;
            color: var(--accent);
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        a {
            color: var(--accent);
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        .note {
            font-size: 11px;
            color: var(--muted);
            font-style: italic;
        }
        
        .references {
            font-size: 12px;
            line-height: 1.8;
        }
        
        .references li {
            margin-bottom: 8px;
            padding-left: 8px;
        }
        
        footer {
            margin-top: 64px;
            padding-top: 32px;
            border-top: 1px solid var(--border);
            font-size: 12px;
            color: var(--muted);
        }
        
        .algorithm {
            border: 1px solid var(--border);
            margin: 32px 0;
            background: white;
        }
        
        .algorithm-header {
            background: var(--code-bg);
            padding: 12px 16px;
            font-size: 12px;
            font-weight: 600;
            border-bottom: 1px solid var(--border);
        }
        
        .algorithm-body {
            padding: 16px;
            font-size: 12px;
        }
        
        .algorithm-body .line {
            margin: 4px 0;
            padding-left: 24px;
            position: relative;
        }
        
        .algorithm-body .line-num {
            position: absolute;
            left: 0;
            color: var(--muted);
            font-size: 10px;
        }
    </style>
</head>

<body>
    <div class="container">
        <header>
            <div class="tag">Reinforcement Learning · JAX · Combinatorial Optimization</div>
            <h1>Expert Iteration for 3D Bin Packing</h1>
            <p class="subtitle">
                How to train AlphaZero-style policies for single-agent planning problems using JAX parallelism. No self-play required—just MCTS as an expert teacher.
            </p>
            <div class="meta">
                <a href="https://aneeshers.github.io">Aneesh Muppidi</a> · Harvard · February 2025
            </div>
            <div class="buttons">
                <a href="https://github.com/Aneeshers/expert-iteration-rl" class="btn btn-primary" target="_blank">
                    GitHub
                </a>
                <a href="https://github.com/Aneeshers/expert-iteration-rl/blob/main/train_expert_iteration_binpack.py" class="btn" target="_blank">
                    Expert Iteration Code
                </a>
                <a href="https://github.com/Aneeshers/expert-iteration-rl/blob/main/train_ppo_binpack.py" class="btn" target="_blank">
                    PPO Baseline
                </a>
            </div>
        </header>

        <figure class="visual-abstract">
            <img src="alphazero_binpack.gif" alt="Expert Iteration for 3D Bin Packing">
            <figcaption>Visual Abstract: MCTS guides item placement decisions in real-time</figcaption>
        </figure>

        <div class="tldr">
            <div class="tldr-label">TL;DR</div>
            <p>
                Expert Iteration combines MCTS with neural network distillation to solve planning problems. With JAX's <code class="inline-code">vmap</code> and <code class="inline-code">pmap</code>, we can run MCTS fast enough to compete with PPO on
                wall-clock time—while achieving
                <strong>96% volume utilization</strong> vs PPO's 90% on 3D bin packing.
            </p>
        </div>

        <h2>The Problem: 3D Bin Packing</h2>

        <p>
            I first encountered bin packing in Harvard's
            <a href="https://docs.google.com/document/d/1zr1xozlsfoF0hhABF46xvXZhK62Jy18x/edit" target="_blank">CS 124</a> (Data Structures and Algorithms), where it appeared in a collection of NP-hard problems. The setup is deceptively simple: given
            a set of rectangular boxes and a container, pack as many boxes as possible to maximize volume utilization.
        </p>

        <p>
            In its 3D form, this is equivalent to the
            <a href="https://en.wikipedia.org/wiki/Knapsack_problem" target="_blank">3D Knapsack Problem</a>—one of Karp's 21 NP-complete problems. The challenge isn't just fitting boxes; it's the combinatorial explosion of possible placements. With <em>n</em>            items and <em>m</em> potential positions, the search space is roughly O(m<sup>n</sup>).
        </p>

        <div class="highlight-box">
            <h4>Why is 3D Bin Packing NP-Hard?</h4>
            <p>
                The 1D bin packing problem is already NP-hard (reducible from
                <a href="https://en.wikipedia.org/wiki/Partition_problem" target="_blank">PARTITION</a>). Adding dimensions only makes it harder. There's no known polynomial-time algorithm that finds optimal solutions, which is why heuristics and learning-based
                approaches are so valuable.
            </p>
            <p style="margin-top: 12px;">
                <strong>Reference:</strong> Garey & Johnson,
                <a href="https://perso.limos.fr/~palafour/PAPERS/PDF/Garey-Johnson79.pdf" target="_blank">
                    <em>Computers and Intractability</em></a>, Chapter 3.
            </p>
        </div>

        <h3>The Jumanji Environment</h3>

        <p>
            We use InstaDeep's <a href="https://github.com/instadeepai/jumanji" target="_blank">Jumanji</a> library, which provides a JAX-native BinPack environment. The key abstraction is
            <strong>Empty Maximal Spaces (EMS)</strong>: rectangular regions inside the container where items can be placed.
        </p>

        <p>
            At each step, the agent chooses:
        </p>
        <ul>
            <li><strong>Which EMS</strong> to place an item in (from up to 40 candidates)</li>
            <li><strong>Which item</strong> to place (from up to 20 items)</li>
        </ul>

        <p>
            This gives a joint action space of 40 × 20 = 800 discrete actions. The environment uses
            <strong>dense rewards</strong>: each placement adds the item's volume (normalized by container volume) to the cumulative return. A perfect packing yields a return of 1.0.
        </p>

        <pre><code><span class="comment"># Environment setup (from our implementation)</span>
<span class="keyword">import</span> jumanji

env = jumanji.make(<span class="string">"BinPack-v2"</span>)

<span class="comment"># Action space dimensions</span>
obs_num_ems = <span class="number">40</span>   <span class="comment"># Observable empty maximal spaces</span>
max_num_items = <span class="number">20</span>  <span class="comment"># Maximum items per episode</span>
num_actions = obs_num_ems * max_num_items  <span class="comment"># 800 total</span>

<span class="comment"># Flatten action for simpler MCTS handling</span>
<span class="keyword">def</span> <span class="function">unflatten_action</span>(action):
    <span class="string">"""Flat index → (ems_id, item_id)"""</span>
    ems_id = action // max_num_items
    item_id = action % max_num_items
    <span class="keyword">return</span> jnp.stack([ems_id, item_id], axis=-<span class="number">1</span>)</code></pre>

        <p class="note">
            Full environment code:
            <a href="https://github.com/instadeepai/jumanji/blob/main/jumanji/environments/packing/bin_pack/env.py" target="_blank">
            jumanji/environments/packing/bin_pack/env.py</a>
        </p>

        <h2>Expert Iteration: Not Self-Play</h2>

        <div class="insight-box">
            <strong>Key Insight</strong>
            <p>
                Unlike board games (Go, Chess), 3D bin packing is a <em>single-agent</em> planning task. There's no opponent, so we don't need self-play. Instead, we use MCTS to generate improved policy targets, then distill that knowledge into a neural
                network.
            </p>
            <p style="margin-top: 12px;">
                This is <strong>Expert Iteration</strong>—the MCTS acts as an "expert" that teaches the network.
            </p>
            <pre>
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│   ┌─────────┐      improved       ┌─────────┐      distill      ┌───────┐   │
│   │  MCTS   │ ───────────────────►│ Policy  │ ─────────────────►│  NN   │   │
│   │ Search  │      targets        │ Targets │      into         │Policy │   │
│   └─────────┘                     └─────────┘                   └───────┘   │
│        ▲                                                            │       │
│        │                          next iteration                    │       │
│        └────────────────────────────────────────────────────────────┘       │
│                                                                             │
│   The network improves → MCTS becomes stronger → better targets → ...       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
</pre>
            <p style="margin-top: 12px;">
                <strong>Paper:</strong> Anthony et al.,
                <a href="https://arxiv.org/abs/1705.08439" target="_blank">"Thinking Fast and Slow with Deep Learning and Tree Search"</a> (2017)
            </p>
        </div>

        <h3>Why Expert Iteration Beats PPO</h3>

        <p>
            PPO learns from <em>sampled actions</em>—single draws from the policy distribution. Expert Iteration learns from <em>full MCTS distributions</em>—a much richer training signal.
        </p>

        <div class="comparison">
            <div class="comparison-item">
                <h4>PPO</h4>
                <ul>
                    <li>Samples single action from π(a|s)</li>
                    <li>Learns from scalar reward signal</li>
                    <li>Policy gradient has high variance</li>
                    <li>No lookahead planning</li>
                </ul>
            </div>
            <div class="comparison-item winner">
                <h4>Expert Iteration ✓</h4>
                <ul>
                    <li>Uses full MCTS action distribution</li>
                    <li>Cross-entropy loss (richer signal)</li>
                    <li>Search provides implicit variance reduction</li>
                    <li>Lookahead with exact dynamics</li>
                </ul>
            </div>
        </div>

        <p>
            The key insight is that we have <strong>perfect knowledge of the environment dynamics</strong>. MCTS can simulate exact state transitions—no need to learn a world model. This is a huge advantage for combinatorial problems where the rules are
            known but the optimal strategy is hard.
        </p>

        <h2>The Algorithm</h2>

        <div class="algorithm">
            <div class="algorithm-header">Algorithm 1: Expert Iteration for BinPack</div>
            <div class="algorithm-body">
                <div class="line"><span class="line-num">1</span><strong>for</strong> iteration = 1 to N <strong>do</strong></div>
                <div class="line"><span class="line-num">2</span>&nbsp;&nbsp;<span class="comment">// Collect experience with MCTS</span></div>
                <div class="line"><span class="line-num">3</span>&nbsp;&nbsp;<strong>for</strong> each episode in batch <strong>do</strong></div>
                <div class="line"><span class="line-num">4</span>&nbsp;&nbsp;&nbsp;&nbsp;<strong>for</strong> each step <strong>do</strong></div>
                <div class="line"><span class="line-num">5</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Run MCTS from current state</div>
                <div class="line"><span class="line-num">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Record (observation, MCTS_policy, reward)</div>
                <div class="line"><span class="line-num">7</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Execute action sampled from MCTS policy</div>
                <div class="line"><span class="line-num">8</span>&nbsp;&nbsp;<span class="comment">// Compute value targets</span></div>
                <div class="line"><span class="line-num">9</span>&nbsp;&nbsp;value_targets ← Monte Carlo returns</div>
                <div class="line"><span class="line-num">10</span>&nbsp;&nbsp;<span class="comment">// Train network</span></div>
                <div class="line"><span class="line-num">11</span>&nbsp;&nbsp;<strong>for</strong> each minibatch <strong>do</strong></div>
                <div class="line"><span class="line-num">12</span>&nbsp;&nbsp;&nbsp;&nbsp;L = CrossEntropy(π_θ, MCTS_policy) + MSE(V_θ, value_targets)</div>
                <div class="line"><span class="line-num">13</span>&nbsp;&nbsp;&nbsp;&nbsp;θ ← θ - α∇L</div>
            </div>
        </div>

        <h3>MCTS with DeepMind's mctx</h3>

        <p>
            We use <a href="https://github.com/google-deepmind/mctx" target="_blank">mctx</a>, DeepMind's JAX-native MCTS library. The key function is <code class="inline-code">gumbel_muzero_policy</code>, which uses Gumbel-Top-k sampling for better exploration
            during search.
        </p>

        <pre><code><span class="keyword">import</span> mctx

<span class="comment"># MCTS requires a "recurrent function" that simulates environment transitions</span>
<span class="keyword">def</span> <span class="function">recurrent_fn</span>(model, rng_key, action, state):
    <span class="string">"""Environment model for MCTS simulation."""</span>
    <span class="comment"># Step environment (we have perfect dynamics!)</span>
    action_pair = unflatten_action(action)
    next_state, timestep = jax.vmap(env.step)(state, action_pair)
    
    <span class="comment"># Get network predictions for next state</span>
    logits, value = forward.apply(model, timestep.observation)
    
    <span class="keyword">return</span> mctx.RecurrentFnOutput(
        reward=timestep.reward,
        discount=timestep.discount,
        prior_logits=logits,
        value=value,
    ), next_state

<span class="comment"># Run MCTS from current state</span>
policy_output = mctx.gumbel_muzero_policy(
    params=model,
    rng_key=key,
    root=mctx.RootFnOutput(
        prior_logits=network_logits,
        value=network_value,
        embedding=current_state,  <span class="comment"># State IS the embedding</span>
    ),
    recurrent_fn=recurrent_fn,
    num_simulations=<span class="number">32</span>,          <span class="comment"># Tree search budget</span>
    invalid_actions=~valid_mask,
)

<span class="comment"># Extract improved policy (this is our training target!)</span>
mcts_policy = policy_output.action_weights  <span class="comment"># Shape: (batch, num_actions)</span></code></pre>

        <p class="note">
            The <code class="inline-code">action_weights</code> output is a probability distribution over actions, computed from MCTS visit counts. This is much richer than a single sampled action.
        </p>

        <h2>JAX Parallelism: The Secret Sauce</h2>

        <p>
            The historical challenge with Expert Iteration is that MCTS is slow. Running 32 simulations per action adds significant overhead. But JAX's parallelization primitives change the calculus:
        </p>

        <div class="highlight-box">
            <h4>jax.vmap: Vectorize Over Batch</h4>
            <p>
                Automatically parallelizes operations across the batch dimension. One line of code turns a single-episode function into a batched version:
            </p>
            <pre><code><span class="comment"># Instead of looping over episodes...</span>
<span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):
    state, ts = env.step(states[i], actions[i])

<span class="comment"># ...vectorize with vmap</span>
state, ts = jax.vmap(env.step)(states, actions)  <span class="comment"># All episodes in parallel!</span></code></pre>
        </div>

        <div class="highlight-box">
            <h4>jax.pmap: Distribute Across Devices</h4>
            <p>
                Replicates computation across all available GPUs/TPUs. Gradients are synchronized automatically with <code class="inline-code">jax.lax.pmean</code>:
            </p>
            <pre><code><span class="decorator">@partial</span>(jax.pmap, axis_name=<span class="string">"devices"</span>)
<span class="keyword">def</span> <span class="function">train_step</span>(model, opt_state, batch):
    grads = jax.grad(loss_fn)(model, batch)
    
    <span class="comment"># Sync gradients across all devices</span>
    grads = jax.lax.pmean(grads, axis_name=<span class="string">"devices"</span>)
    
    <span class="comment"># Apply optimizer update</span>
    updates, opt_state = optimizer.update(grads, opt_state)
    model = optax.apply_updates(model, updates)
    <span class="keyword">return</span> model, opt_state</code></pre>
        </div>

        <p>
            Together, vmap and pmap give us massive throughput. On 4 GPUs with batch size 1024, we process ~20,000 MCTS-guided decisions per second—competitive with PPO's sample efficiency.
        </p>

        <p class="note">
            <strong>Further reading:</strong>
            <a href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html" target="_blank">JAX Parallelism Tutorial</a>
        </p>

        <h2>Neural Network Architecture</h2>

        <p>
            We adapt Jumanji's
            <a href="https://github.com/instadeepai/jumanji/blob/main/jumanji/training/networks/bin_pack/actor_critic.py" target="_blank">A2C architecture</a> for our policy-value network. The key insight is using <strong>cross-attention</strong> between
            EMS tokens and item tokens.
        </p>

        <pre><code><span class="comment"># Architecture overview</span>
<span class="comment">#</span>
<span class="comment">#   ┌─────────────┐     ┌─────────────┐</span>
<span class="comment">#   │  EMS tokens │     │ Item tokens │</span>
<span class="comment">#   └──────┬──────┘     └──────┬──────┘</span>
<span class="comment">#          │                   │</span>
<span class="comment">#          ▼                   ▼</span>
<span class="comment">#   ┌─────────────┐     ┌─────────────┐</span>
<span class="comment">#   │ Self-Attn   │     │ Self-Attn   │</span>
<span class="comment">#   └──────┬──────┘     └──────┬──────┘</span>
<span class="comment">#          │                   │</span>
<span class="comment">#          └────────┬──────────┘</span>
<span class="comment">#                   ▼</span>
<span class="comment">#          ┌───────────────┐</span>
<span class="comment">#          │ Cross-Attn    │  ← EMS ↔ Items interaction</span>
<span class="comment">#          │ (bidirectional)│</span>
<span class="comment">#          └───────┬───────┘</span>
<span class="comment">#                  │</span>
<span class="comment">#          ┌───────┴───────┐</span>
<span class="comment">#          ▼               ▼</span>
<span class="comment">#   ┌─────────────┐  ┌─────────────┐</span>
<span class="comment">#   │ Policy Head │  │ Value Head  │</span>
<span class="comment">#   │ (E×I logits)│  │ (scalar)    │</span>
<span class="comment">#   └─────────────┘  └─────────────┘</span>

<span class="keyword">class</span> <span class="function">BinPackEncoder</span>(hk.Module):
    <span class="keyword">def</span> <span class="function">__call__</span>(self, observation):
        <span class="comment"># Embed EMS coordinates: (x1,x2,y1,y2,z1,z2) → model_size</span>
        ems_emb = self._embed_ems(observation.ems)
        items_emb = self._embed_items(observation.items)
        
        <span class="keyword">for</span> layer <span class="keyword">in</span> range(self.num_layers):
            <span class="comment"># Self-attention within each token type</span>
            ems_emb = TransformerBlock(...)(ems_emb, ems_emb, ems_emb)
            items_emb = TransformerBlock(...)(items_emb, items_emb, items_emb)
            
            <span class="comment"># Cross-attention: EMS ↔ Items (gated by action_mask)</span>
            ems_emb = TransformerBlock(...)(ems_emb, items_emb, items_emb)
            items_emb = TransformerBlock(...)(items_emb, ems_emb, ems_emb)
        
        <span class="keyword">return</span> ems_emb, items_emb</code></pre>

        <p>
            The <strong>policy head</strong> computes logits via a bilinear form:
            <code class="inline-code">logits[e,i] = ems_h[e] · items_h[i]</code>. This naturally produces the (E × I) shaped output we need.
        </p>

        <p>
            The <strong>value head</strong> pools embeddings and predicts expected utilization ∈ [0, 1].
        </p>

        <h2>Training Loop</h2>

        <p>
            The full training loop follows the standard Expert Iteration pattern: collect → compute targets → train.
        </p>

        <pre><code><span class="keyword">for</span> iteration <span class="keyword">in</span> range(max_iterations):
    <span class="comment"># ─────────────────────────────────────────────────────────────</span>
    <span class="comment"># Step 1: Collect experience with MCTS-guided rollouts</span>
    <span class="comment"># ─────────────────────────────────────────────────────────────</span>
    rollout_data = collect_experience(model, rng_key)
    <span class="comment"># rollout_data contains: (observations, mcts_policies, rewards)</span>
    
    <span class="comment"># ─────────────────────────────────────────────────────────────</span>
    <span class="comment"># Step 2: Compute Monte-Carlo value targets</span>
    <span class="comment"># ─────────────────────────────────────────────────────────────</span>
    training_samples = compute_value_targets(rollout_data)
    <span class="comment"># V(t) = r(t) + γ·r(t+1) + γ²·r(t+2) + ...</span>
    
    <span class="comment"># ─────────────────────────────────────────────────────────────</span>
    <span class="comment"># Step 3: Train network to match MCTS policy and value</span>
    <span class="comment"># ─────────────────────────────────────────────────────────────</span>
    <span class="keyword">for</span> minibatch <span class="keyword">in</span> shuffle_and_batch(training_samples):
        model, opt_state = train_step(model, opt_state, minibatch)
        
        <span class="comment"># Loss = CE(π_θ, mcts_policy) + λ·MSE(V_θ, value_target)</span></code></pre>

        <p class="note">
            Full implementation:
            <a href="https://github.com/Aneeshers/expert-iteration-rl/blob/main/train_expert_iteration_binpack.py" target="_blank">
            train_expert_iteration_binpack.py</a>
        </p>

        <h2>Experiments</h2>

        <p>
            We trained both Expert Iteration and PPO on BinPack-v2 for 800 iterations with 5 random seeds. Both methods use the same network architecture and batch sizes for fair comparison.
        </p>

        <table>
            <tr>
                <th>Hyperparameter</th>
                <th>Expert Iteration</th>
                <th>PPO</th>
            </tr>
            <tr>
                <td>Rollout batch size</td>
                <td>1024</td>
                <td>1024</td>
            </tr>
            <tr>
                <td>Training batch size</td>
                <td>4096</td>
                <td>4096</td>
            </tr>
            <tr>
                <td>MCTS simulations</td>
                <td>32</td>
                <td>—</td>
            </tr>
            <tr>
                <td>PPO epochs</td>
                <td>—</td>
                <td>4</td>
            </tr>
            <tr>
                <td>Learning rate</td>
                <td>1e-3</td>
                <td>3e-4</td>
            </tr>
            <tr>
                <td>Transformer layers</td>
                <td>4</td>
                <td>4</td>
            </tr>
        </table>

        <figure>
            <img src="ppo_vs_alphazero.gif" alt="PPO vs Expert Iteration learning curves">
            <figcaption>
                <strong>Figure 1.</strong> Learning curves comparing Expert Iteration (light blue) vs PPO (dark blue) on BinPack-v2. Shaded regions show ±1 standard error across 5 seeds. Expert Iteration converges to 96% volume utilization; PPO plateaus
                around 90%.
            </figcaption>
        </figure>

        <div class="findings">
            <div class="finding">
                <div class="finding-num">Result 1</div>
                <strong>Expert Iteration achieves 96% utilization.</strong> This represents a 6 percentage point improvement over PPO, translating to significantly better packing efficiency.
            </div>
            <div class="finding">
                <div class="finding-num">Result 2</div>
                <strong>Comparable wall-clock time.</strong> Despite running 32 MCTS simulations per action, JAX parallelism keeps training time competitive with PPO.
            </div>
            <div class="finding">
                <div class="finding-num">Result 3</div>
                <strong>More stable convergence.</strong> Expert Iteration shows tighter variance bands across seeds, suggesting the MCTS-generated targets provide more consistent supervision.
            </div>
        </div>

        <h2>Key Implementation Details</h2>

        <h3>jax.lax.scan for Efficient Loops</h3>

        <p>
            Python loops inside JIT-compiled functions are unrolled at compile time, causing slow compilation and memory issues. Use <code class="inline-code">jax.lax.scan</code> instead:
        </p>

        <pre><code><span class="comment"># BAD: Python loop (slow to compile)</span>
<span class="keyword">def</span> <span class="function">rollout_bad</span>(state, keys):
    trajectory = []
    <span class="keyword">for</span> key <span class="keyword">in</span> keys:
        state, data = step(state, key)
        trajectory.append(data)
    <span class="keyword">return</span> trajectory

<span class="comment"># GOOD: jax.lax.scan (compiles once, runs fast)</span>
<span class="keyword">def</span> <span class="function">rollout_good</span>(state, keys):
    <span class="keyword">def</span> <span class="function">step_fn</span>(carry, key):
        state = carry
        state, data = step(state, key)
        <span class="keyword">return</span> state, data
    
    final_state, trajectory = jax.lax.scan(step_fn, state, keys)
    <span class="keyword">return</span> trajectory</code></pre>

        <h3>Haiku for Stateful Networks</h3>

        <p>
            We use <a href="https://dm-haiku.readthedocs.io/" target="_blank">Haiku</a> for neural networks. The <code class="inline-code">hk.transform_with_state</code> pattern separates pure functions from state:
        </p>

        <pre><code><span class="keyword">import</span> haiku <span class="keyword">as</span> hk

<span class="keyword">def</span> <span class="function">forward_fn</span>(observation, is_eval=<span class="keyword">False</span>):
    net = BinPackPolicyValueNet(...)
    <span class="keyword">return</span> net(observation, is_training=<span class="keyword">not</span> is_eval)

<span class="comment"># Transform to pure functions</span>
forward = hk.without_apply_rng(hk.transform_with_state(forward_fn))

<span class="comment"># Initialize</span>
params, state = forward.init(rng_key, dummy_obs)

<span class="comment"># Apply (pure function!)</span>
(logits, value), new_state = forward.apply(params, state, observation)</code></pre>

        <h3>Action Masking</h3>

        <p>
            Invalid actions (item doesn't fit in EMS, item already placed) must be masked. We use a large negative value instead of -∞ to avoid NaN in softmax:
        </p>

        <pre><code><span class="keyword">def</span> <span class="function">apply_action_mask</span>(logits, valid_mask):
    <span class="string">"""Set invalid action logits to large negative value."""</span>
    <span class="comment"># Center for numerical stability</span>
    logits = logits - jnp.max(logits, axis=-<span class="number">1</span>, keepdims=<span class="keyword">True</span>)
    <span class="comment"># Use finite minimum (not -inf) to avoid NaN</span>
    <span class="keyword">return</span> jnp.where(valid_mask, logits, jnp.finfo(logits.dtype).min)</code></pre>

        <h2>When to Use Expert Iteration</h2>

        <p>
            Expert Iteration shines when:
        </p>
        <ul>
            <li><strong>You have exact dynamics.</strong> MCTS needs to simulate state transitions accurately.</li>
            <li><strong>Episodes are short.</strong> MCTS overhead compounds with horizon length.</li>
            <li><strong>Actions have complex dependencies.</strong> MCTS can explore combinatorial interactions that gradient signals miss.</li>
            <li><strong>You have parallel compute.</strong> vmap/pmap amortize MCTS cost across batches.</li>
        </ul>

        <p>
            PPO might be preferred when:
        </p>
        <ul>
            <li>Dynamics are learned (model-free setting)</li>
            <li>Very long horizons (100s of steps)</li>
            <li>Simple action spaces with clear reward gradients</li>
        </ul>

        <h2>Resources & References</h2>

        <h3>Code</h3>
        <ul>
            <li><a href="https://github.com/Aneeshers/expert-iteration-rl" target="_blank">Our implementation (GitHub)</a></li>
            <li><a href="https://github.com/google-deepmind/mctx" target="_blank">mctx: JAX MCTS library (DeepMind)</a></li>
            <li><a href="https://github.com/sotetsuk/pgx/tree/main/examples/alphazero" target="_blank">PGX AlphaZero (reference implementation)</a></li>
            <li><a href="https://github.com/instadeepai/jumanji" target="_blank">Jumanji environments (InstaDeep)</a></li>
        </ul>

        <h3>Papers</h3>
        <ol class="references">
            <li>
                Anthony et al.,
                <a href="https://arxiv.org/abs/1705.08439" target="_blank">"Thinking Fast and Slow with Deep Learning and Tree Search"</a>, NeurIPS 2017. <em>(Expert Iteration)</em>
            </li>
            <li>
                Silver et al.,
                <a href="https://www.nature.com/articles/nature24270" target="_blank">"Mastering the game of Go without human knowledge"</a>, Nature 2017. <em>(AlphaZero)</em>
            </li>
            <li>
                Schrittwieser et al.,
                <a href="https://arxiv.org/abs/1911.08265" target="_blank">"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"</a>, Nature 2020. <em>(MuZero)</em>
            </li>
            <li>
                Schulman et al.,
                <a href="https://arxiv.org/abs/1707.06347" target="_blank">"Proximal Policy Optimization Algorithms"</a>, arXiv 2017. <em>(PPO)</em>
            </li>
            <li>
                Garey & Johnson,
                <a href="https://www.amazon.com/Computers-Intractability-NP-Completeness-Mathematical-Sciences/dp/0716710455" target="_blank">"Computers and Intractability: A Guide to NP-Completeness"</a>, 1979. <em>(NP-hardness reference)</em>
            </li>
        </ol>

        <h3>Documentation</h3>
        <ul>
            <li><a href="https://jax.readthedocs.io/en/latest/" target="_blank">JAX Documentation</a></li>
            <li><a href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html" target="_blank">JAX Parallelism Tutorial</a></li>
            <li><a href="https://dm-haiku.readthedocs.io/" target="_blank">Haiku Documentation</a></li>
            <li><a href="https://optax.readthedocs.io/" target="_blank">Optax Documentation</a></li>
        </ul>

        <h2>Citation</h2>

        <pre><code>@misc{muppidi2025expertiteration,
  title={Expert Iteration for 3D Bin Packing},
  author={Muppidi, Aneesh},
  year={2025},
  url={https://github.com/Aneeshers/expert-iteration-rl}
}</code></pre>

        <footer>
            <p>
                <a href="https://aneeshers.github.io"><strong>Aneesh Muppidi</strong></a><br> Harvard College · February 2025
            </p>
            <p style="margin-top: 16px;">
                <a href="https://github.com/Aneeshers/expert-iteration-rl">GitHub</a> ·
                <a href="https://twitter.com/aneeshers">Twitter</a>
            </p>
        </footer>
    </div>
</body>

</html>