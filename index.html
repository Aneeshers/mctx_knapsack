<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Policy Iteration via Search Distillation</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <style>
        @font-face {
            font-family: 'Berkeley Mono';
            src: url('BerkeleyMonoTrial-Regular.otf') format('opentype');
            font-weight: normal;
            font-style: normal;
        }
        
         :root {
            --bg: #ffffff;
            --fg: #1a1a1a;
            --accent: #2563eb;
            --accent-light: #dbeafe;
            --code-bg: #f8f9fa;
            --border: #e5e7eb;
            --muted: #6b7280;
            --success: #059669;
            --warn: #b45309;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Berkeley Mono', 'SF Mono', 'Consolas', monospace;
            background: var(--bg);
            color: var(--fg);
            line-height: 1.75;
            font-size: 14px;
        }
        
        .container {
            max-width: 860px;
            margin: 0 auto;
            padding: 60px 24px;
        }
        
        header {
            margin-bottom: 48px;
            border-bottom: 1px solid var(--border);
            padding-bottom: 32px;
        }
        
        .tag {
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: var(--accent);
            margin-bottom: 16px;
        }
        
        h1 {
            font-size: 28px;
            font-weight: 600;
            line-height: 1.3;
            margin-bottom: 20px;
            letter-spacing: -0.5px;
        }
        
        .subtitle {
            font-size: 15px;
            color: var(--muted);
            margin-bottom: 24px;
            line-height: 1.6;
        }
        
        .meta {
            font-size: 12px;
            color: var(--muted);
            margin-bottom: 24px;
        }
        
        .meta a {
            color: var(--fg);
            text-decoration: none;
        }
        
        .meta a:hover {
            color: var(--accent);
        }
        
        .buttons {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
        }
        
        .btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 10px 16px;
            font-family: inherit;
            font-size: 12px;
            text-decoration: none;
            border: 1px solid var(--border);
            background: white;
            color: var(--fg);
            transition: all 0.15s ease;
        }
        
        .btn:hover {
            border-color: var(--accent);
            color: var(--accent);
        }
        
        .btn-primary {
            background: var(--fg);
            color: white;
            border-color: var(--fg);
        }
        
        .btn-primary:hover {
            background: var(--accent);
            border-color: var(--accent);
            color: white;
        }
        
        .visual-abstract {
            margin: 48px 0;
            text-align: center;
        }
        
        .visual-abstract img {
            max-width: 100%;
            border: 1px solid var(--border);
        }
        
        .visual-abstract figcaption {
            font-size: 11px;
            color: var(--muted);
            margin-top: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .tldr {
            background: var(--code-bg);
            padding: 20px 24px;
            margin: 32px 0;
            border: 1px solid var(--border);
        }
        
        .tldr-label {
            font-size: 10px;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: var(--accent);
            margin-bottom: 8px;
            font-weight: 600;
        }
        
        .tldr p {
            margin: 0;
            font-size: 13px;
        }
        
        .toc {
            background: white;
            padding: 18px 22px;
            margin: 24px 0 40px 0;
            border: 1px solid var(--border);
        }
        
        .toc .toc-title {
            font-size: 10px;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: var(--muted);
            margin-bottom: 10px;
            font-weight: 600;
        }
        
        .toc ul {
            margin: 0 0 0 18px;
        }
        
        .toc li {
            margin: 6px 0;
        }
        
        .insight-box {
            border: 2px solid var(--accent);
            padding: 24px;
            margin: 40px 0;
            background: var(--accent-light);
            font-size: 13px;
        }
        
        .insight-box pre {
            background: white;
            border: 1px solid var(--border);
            padding: 16px;
            margin: 16px 0 0 0;
            font-size: 11px;
            line-height: 1.5;
            white-space: pre;
            overflow-x: auto;
            color: var(--fg);
        }
        
        .callout {
            border: 1px solid var(--border);
            background: white;
            padding: 18px 20px;
            margin: 28px 0;
        }
        
        .callout h4 {
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--accent);
            margin-bottom: 8px;
        }
        
        .callout.warn h4 {
            color: var(--warn);
        }
        
        .callout.warn {
            border-left: 3px solid var(--warn);
        }
        
        h2 {
            font-size: 18px;
            font-weight: 600;
            margin: 48px 0 20px 0;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border);
        }
        
        h3 {
            font-size: 14px;
            font-weight: 600;
            margin: 32px 0 12px 0;
            color: var(--accent);
        }
        
        h4 {
            font-size: 13px;
            font-weight: 600;
            margin: 18px 0 10px 0;
        }
        
        p {
            margin-bottom: 16px;
        }
        
        pre {
            background: #1a1a1a;
            color: #e0e0e0;
            padding: 20px;
            overflow-x: auto;
            margin: 24px 0;
            font-size: 12px;
            line-height: 1.6;
            border-radius: 0;
        }
        
        code {
            font-family: 'Berkeley Mono', monospace;
        }
        
        .inline-code {
            background: var(--code-bg);
            padding: 2px 6px;
            font-size: 12px;
            border: 1px solid var(--border);
        }
        
        .keyword {
            color: #569cd6;
        }
        
        .function {
            color: #dcdcaa;
        }
        
        .string {
            color: #ce9178;
        }
        
        .comment {
            color: #6a9955;
        }
        
        .number {
            color: #b5cea8;
        }
        
        .decorator {
            color: #c586c0;
        }
        
        figure {
            margin: 40px 0;
        }
        
        figure img {
            width: 100%;
            border: 1px solid var(--border);
            background: white;
        }
        
        figcaption {
            font-size: 12px;
            color: var(--muted);
            margin-top: 12px;
            padding: 0 8px;
            line-height: 1.6;
        }
        
        figcaption strong {
            color: var(--fg);
        }
        
        .math-block {
            background: var(--code-bg);
            padding: 20px;
            margin: 24px 0;
            text-align: center;
            border: 1px solid var(--border);
            overflow-x: auto;
        }
        
        ul,
        ol {
            margin: 16px 0 16px 24px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            font-size: 12px;
        }
        
        th,
        td {
            border: 1px solid var(--border);
            padding: 10px 12px;
            text-align: left;
        }
        
        th {
            background: var(--code-bg);
            font-weight: 600;
        }
        
        .highlight-box {
            border-left: 3px solid var(--accent);
            padding: 16px 20px;
            margin: 32px 0;
            background: var(--code-bg);
        }
        
        .highlight-box h4 {
            color: var(--accent);
            margin-bottom: 8px;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 32px 0;
        }
        
        .comparison-item {
            border: 1px solid var(--border);
            padding: 20px;
            background: white;
        }
        
        .comparison-item h4 {
            font-size: 13px;
            margin-bottom: 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border);
        }
        
        .comparison-item.winner {
            border-color: var(--success);
            background: #ecfdf5;
        }
        
        .comparison-item.winner h4 {
            color: var(--success);
        }
        
        @media (max-width: 600px) {
            .comparison {
                grid-template-columns: 1fr;
            }
        }
        
        .findings {
            display: grid;
            gap: 16px;
            margin: 24px 0;
        }
        
        .finding {
            border-left: 3px solid var(--accent);
            padding: 12px 16px;
            background: var(--code-bg);
        }
        
        .finding-num {
            font-size: 10px;
            color: var(--accent);
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        a {
            color: var(--accent);
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        .note {
            font-size: 11px;
            color: var(--muted);
            font-style: italic;
        }
        
        .source-note {
            font-size: 11px;
            color: var(--muted);
            margin-top: -10px;
        }
        
        .references {
            font-size: 12px;
            line-height: 1.8;
        }
        
        .references li {
            margin-bottom: 8px;
            padding-left: 8px;
        }
        
        footer {
            margin-top: 64px;
            padding-top: 32px;
            border-top: 1px solid var(--border);
            font-size: 12px;
            color: var(--muted);
        }
        
        .algorithm {
            border: 1px solid var(--border);
            margin: 32px 0;
            background: white;
        }
        
        .algorithm-header {
            background: var(--code-bg);
            padding: 12px 16px;
            font-size: 12px;
            font-weight: 600;
            border-bottom: 1px solid var(--border);
        }
        
        .algorithm-body {
            padding: 16px;
            font-size: 12px;
        }
        
        .algorithm-body .line {
            margin: 4px 0;
            padding-left: 24px;
            position: relative;
        }
        
        .algorithm-body .line-num {
            position: absolute;
            left: 0;
            color: var(--muted);
            font-size: 10px;
        }
    </style>
</head>

<body>
    <div class="container">
        <header>
            <div class="tag">Search-Based RL · JAX · Combinatorial Optimization</div>
            <h1>Policy Iteration via Search Distillation</h1>
            <p class="subtitle">
                <strong>MCTS as a policy improvement operator.</strong> For single-agent planning with known dynamics, we use Gumbel MuZero search to compute improved policy targets—<code class="inline-code">softmax(prior + Q)</code>—then distill them
                into a neural network via cross-entropy. This is policy iteration, but the "improvement" step is search and the "evaluation" step is supervised learning. Pure JAX, competitive with PPO on wall-clock time.
            </p>

            <div class="meta">
                <a href="https://aneeshers.github.io">Aneesh Muppidi</a> · February 2026
            </div>
            <div class="buttons">
                <a href="https://github.com/Aneeshers/expert-iteration-rl" class="btn btn-primary" target="_blank">
                    GitHub
                </a>
                <a href="https://github.com/Aneeshers/expert-iteration-rl/blob/main/train_expert_iteration_binpack.py" class="btn" target="_blank">
                    Training Code
                </a>
                <a href="https://github.com/Aneeshers/expert-iteration-rl/blob/main/train_ppo_binpack.py" class="btn" target="_blank">
                    PPO Baseline
                </a>
            </div>
        </header>

        <figure class="visual-abstract">
            <img src="alphazero_binpack.gif" alt="Policy Iteration via Search Distillation for 3D Bin Packing">
            <figcaption>Left: The 3D bin packing problem. Fit as many items as possible into a fixed container to maximize volume utilization. Items are placed one at a time; opacity shows placement order (newer = more solid). Right: MCTS explores possible placements
                before each decision, with node size indicating visit count.
            </figcaption>
        </figure>

        <div class="tldr">
            <div class="tldr-label">TL;DR</div>
            <p>
                Policy iteration, but the improvement step is Gumbel MuZero search and the learning step is supervised distillation. With JAX's <code class="inline-code">vmap</code> and <code class="inline-code">pmap</code>, we can run search fast enough
                to compete with PPO on wall-clock time—while achieving <strong>96% volume utilization</strong> vs PPO's 90% on 3D bin packing. Closely related to Expert Iteration, but with important differences we make explicit.
            </p>
        </div>

        <div class="toc">
            <div class="toc-title">Contents</div>
            <ul>
                <li><a href="#method">Policy Iteration via Search Distillation</a></li>
                <li><a href="#mcts">Monte Carlo Tree Search with mctx</a></li>
                <li><a href="#gumbel">Gumbel Exploration</a></li>
                <li><a href="#problem">The Problem: 3D Bin Packing</a></li>
                <li><a href="#jumanji">The Jumanji Environment</a></li>
                <li><a href="#jax">JAX Parallelism</a></li>
                <li><a href="#architecture">Neural Network Architecture</a></li>
                <li><a href="#experiments">Experiments</a></li>
                <li><a href="#implementation">Implementation Details</a></li>
                <li><a href="#refs">References</a></li>
            </ul>
        </div>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Policy Iteration via Search Distillation -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="method">Policy Iteration via Search Distillation</h2>

        <p>
            When AlphaZero achieved <a href="https://www.youtube.com/watch?v=Wujy7OzvdJk" target="_blank">superhuman performance at Go and Chess</a>, the key innovation wasn't just MCTS or neural networks—it was how the two components reinforced each
            other. The network provides fast intuition to guide search, and search provides high-quality targets to improve the network. But there's an important detail that's often glossed over: AlphaZero uses <em>self-play</em> because Go and Chess
            are two-player games. The network plays against itself, generating training data from both sides of the board.
        </p>

        <p>
            But many important problems don't have an opponent. Consider 3D bin packing: you're given a set of rectangular boxes and a container, and your goal is to pack as many boxes as possible to maximize volume utilization. It sounds like something you could
            solve by just being clever about placement order, but the combinatorial explosion makes this problem far harder than it appears.
        </p>

        <p>
            In its 3D form, bin packing is equivalent to the <a href="https://en.wikipedia.org/wiki/Knapsack_problem" target="_blank">3D Knapsack Problem</a>—one of Karp's 21 NP-complete problems. (For a thorough treatment of computational complexity
            and bin packing specifically, Garey and Johnson's <a href="https://perso.limos.fr/~palafour/PAPERS/PDF/Garey-Johnson79.pdf" target="_blank"><em>Computers and Intractability</em></a> remains the canonical reference.) The difficulty isn't just
            fitting boxes into the container; it's navigating the astronomical number of possible placements. With <em>n</em> items and <em>m</em> potential positions, the search space grows roughly as O(m<sup>n</sup>). Even for modest problem sizes,
            brute-force search is hopeless.
        </p>

        <p>
            For problems like this, self-play doesn't apply—there's no "other side" to play. Instead, we run search on each state and use the resulting action distribution as a training target. This is closely related to <a href="https://arxiv.org/abs/1705.08439"
                target="_blank">Expert Iteration</a> (Anthony et al., 2017), which frames the idea as "MCTS is an expert teacher, the network is an apprentice."
        </p>

        <p>
            But as we'll see, our implementation differs from vanilla Expert Iteration in important ways—most notably, we use Gumbel MuZero's policy improvement operator rather than visit-count targets. So we prefer a more precise framing: <strong>policy iteration via search distillation</strong>.
        </p>

        <p>
            The algorithm is best understood as <strong>policy iteration</strong>—the classic RL framework where you alternate between evaluating your current policy and improving it. But our implementation of each step is unusual:
        </p>

        <ul>
            <li><strong>Policy improvement:</strong> Run Gumbel MuZero search, which computes <code class="inline-code">softmax(prior + Q)</code> as the improved action distribution</li>
            <li><strong>Policy "evaluation" / learning:</strong> Train the neural network to imitate that distribution via cross-entropy (supervised learning, not policy gradients)</li>
        </ul>

        <p>
            The network gets better, which makes search stronger (since it uses the network for priors and value estimates), which produces even better targets, and so on. This is the core loop:
        </p>

        <pre>
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│   ┌─────────┐      improved       ┌─────────┐      distill      ┌───────┐   │
│   │ Gumbel  │ ───────────────────►│ Policy  │ ─────────────────►│  NN   │   │
│   │ MuZero  │      targets        │ Targets │      into         │Policy │   │
│   └─────────┘                     └─────────┘                   └───────┘   │
│        ▲                                                            │       │
│        │                          next iteration                    │       │
│        └────────────────────────────────────────────────────────────┘       │
│                                                                             │
│   The network improves → search becomes stronger → better targets → ...     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
        </pre>

        <h3>Relationship to Expert Iteration</h3>

        <p>
            This approach is closely related to <a href="https://arxiv.org/abs/1705.08439" target="_blank">Expert Iteration</a> (Anthony et al., 2017), which frames the same idea as "MCTS is an expert teacher, the network is an apprentice." The ExIt paper's
            Appendix A even notes that AlphaGo Zero "independently developed the EXIT algorithm." So the lineage is clear.
        </p>

        <p>
            However, there are meaningful differences that make "Policy Iteration via Search Distillation" a more precise description of what we're actually doing:
        </p>

        <div class="callout">
            <h4>How This Differs from Vanilla Expert Iteration</h4>
            <p><strong>1. Policy targets are not visit counts.</strong> Traditional ExIt uses <code class="inline-code">n(s,a) / n(s)</code>—the proportion of times MCTS visited each action. We use Gumbel MuZero, which computes <code class="inline-code">softmax(prior_logits + completed_Q)</code>.
                This is a direct policy improvement operator with finite-sample guarantees, not an asymptotic property of visit counts.</p>

            <p><strong>2. The "expert" is not independent.</strong> In classical ExIt framing, the expert is often conceptualized as a separate, stronger system. Here, the expert <em>is</em> the current network + search—they're coupled. MCTS uses the network's
                priors to guide search and the network's value estimates to evaluate leaves. This is closer to "compute-amplified self-improvement" than "learning from an external expert."</p>

            <p><strong>3. Value targets come from Monte Carlo returns, not MCTS.</strong> The value head is trained via supervised regression to empirical returns from rollouts—not from MCTS root values or backed-up estimates. So only the <em>policy</em>                is "search-improved"; the value is trained by standard Monte Carlo.</p>

            <p><strong>4. Single-agent, no self-play.</strong> AlphaZero uses self-play because Go and Chess are two-player games. Our target domain (bin packing) is single-agent—there's no opponent. We simply run search on the current state and distill
                the result.</p>
        </div>

        <p>
            None of these differences make the approach invalid—they just mean the ExIt framing is slightly imprecise. "Policy Iteration via Search Distillation" captures the mechanics more accurately: search is the improvement operator, supervised learning is how
            we internalize the improvement.
        </p>

        <h3>Why This Is Not PPO (or Any Policy Gradient Method)</h3>

        <p>
            It's worth being explicit about what kind of algorithm this is and isn't. PPO and other policy gradient methods train the policy directly from reward:
        </p>

        <div class="math-block">
            \[ \nabla_\theta J(\theta) = \mathbb{E}\left[ \nabla_\theta \log \pi_\theta(a|s) \cdot \hat{A}(s,a) \right] \]
        </div>

        <p>
            The gradient signal comes from <em>sampled actions</em> and <em>sampled advantages</em>. This is high-variance, which is why PPO needs clipping, GAE, entropy bonuses, and careful hyperparameter tuning.
        </p>

        <p>
            Our approach is fundamentally different. The policy is trained by <strong>imitation</strong>:
        </p>

        <div class="math-block">
            \[ \mathcal{L}_{\text{policy}}(\theta) = -\sum_{a} \pi_{\text{search}}(a|s) \log \pi_\theta(a|s) \]
        </div>

        <p>
            This is cross-entropy between the network's output and the search-produced distribution. Reward doesn't appear in this loss at all—it only influences the policy <em>indirectly</em> through how search evaluates actions. The supervision signal
            is dense (a full distribution over actions) rather than sparse (a single sampled action and its return).
        </p>

        <p>
            So where does reward come in? Two places:
        </p>

        <ul>
            <li><strong>Inside search:</strong> MCTS uses <code class="inline-code">reward + discount * value</code> to evaluate nodes. Reward shapes which actions search prefers.</li>
            <li><strong>Value head training:</strong> The value network is trained to predict Monte Carlo returns, which are cumulative discounted rewards.</li>
        </ul>

        <p>
            But the policy gradient is purely imitation of search. This is why the learning curves are so stable compared to PPO—we're doing supervised learning with high-quality targets, not high-variance policy gradients.
        </p>

        <h3>The Algorithm, Precisely</h3>

        <p>
            To be completely explicit about what we're doing:
        </p>

        <div class="algorithm">
            <div class="algorithm-header">Algorithm: Policy Iteration via Search Distillation</div>
            <div class="algorithm-body">
                <div class="line"><span class="line-num">1:</span> Initialize policy-value network π<sub>θ</sub>, V<sub>θ</sub></div>
                <div class="line"><span class="line-num">2:</span> <strong>for</strong> iteration = 1, 2, ... <strong>do</strong></div>
                <div class="line"><span class="line-num">3:</span> &nbsp;&nbsp;// Collect experience with search-guided rollouts</div>
                <div class="line"><span class="line-num">4:</span> &nbsp;&nbsp;<strong>for</strong> each episode <strong>do</strong></div>
                <div class="line"><span class="line-num">5:</span> &nbsp;&nbsp;&nbsp;&nbsp;<strong>for</strong> each step <strong>do</strong></div>
                <div class="line"><span class="line-num">6:</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Run Gumbel MuZero search from current state</div>
                <div class="line"><span class="line-num">7:</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Record (state, action_weights, reward)</div>
                <div class="line"><span class="line-num">8:</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample action from action_weights, step environment</div>
                <div class="line"><span class="line-num">9:</span> &nbsp;&nbsp;// Compute targets</div>
                <div class="line"><span class="line-num">10:</span> &nbsp;&nbsp;policy_targets ← action_weights from search</div>
                <div class="line"><span class="line-num">11:</span> &nbsp;&nbsp;value_targets ← Monte Carlo returns from rewards</div>
                <div class="line"><span class="line-num">12:</span> &nbsp;&nbsp;// Update network (supervised learning)</div>
                <div class="line"><span class="line-num">13:</span> &nbsp;&nbsp;θ ← θ - α∇<sub>θ</sub>[CrossEntropy(π<sub>θ</sub>, policy_targets) + MSE(V<sub>θ</sub>, value_targets)]</div>
            </div>
        </div>

        <p>
            The key insight: <strong>search is the policy improvement operator</strong>. Given any policy π, running Gumbel MuZero search produces a better policy π' (under mild conditions). Training the network to match π' internalizes that improvement.
            Repeat.
        </p>

        <h3>Comparison to PPO</h3>

        <p>
            It's worth understanding exactly why this approach outperforms PPO on planning tasks. PPO is an on-policy policy gradient method that maximizes a clipped surrogate objective:
        </p>

        <div class="math-block">
            \[ L^{\text{CLIP}}(\theta)=\mathbb{E}_t\left[\min\left(r_t(\theta)\,\hat{A}_t,\; \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\,\hat{A}_t\right)\right] \]
        </div>

        <p>
            where \(r_t(\theta) = \pi_\theta(a_t|s_t) / \pi_{\theta_{\text{old}}}(a_t|s_t)\) is the probability ratio. PPO works well when you can collect lots of trajectories and the environment is too complex for planning. But PPO's gradient signal is based on
            <em>sampled actions</em> and <em>sampled returns</em>, so variance control (GAE, baselines, clipping) becomes essential.
        </p>

        <p>
            Search distillation sidesteps this variance problem entirely. Instead of learning from a single sampled action and its noisy return, we learn from the full search distribution—a much richer signal. And because we have exact environment dynamics, search
            can do genuine lookahead planning, discovering good action sequences that pure policy gradients might take much longer to find.
        </p>

        <table>
            <tr>
                <th>Aspect</th>
                <th>PPO</th>
                <th>Search Distillation</th>
            </tr>
            <tr>
                <td>Policy learning signal</td>
                <td>Sampled action + advantage</td>
                <td>Full distribution from search</td>
            </tr>
            <tr>
                <td>Reward's role in policy</td>
                <td>Direct (policy gradient)</td>
                <td>Indirect (shapes search preferences)</td>
            </tr>
            <tr>
                <td>Variance</td>
                <td>High (needs GAE, clipping)</td>
                <td>Low (supervised learning)</td>
            </tr>
            <tr>
                <td>Lookahead</td>
                <td>None (reactive)</td>
                <td>Multiple simulations per decision</td>
            </tr>
            <tr>
                <td>Requires dynamics</td>
                <td>No</td>
                <td>Yes (for search)</td>
            </tr>
        </table>

        <p>
            The practical difference is stark on planning-heavy domains: search distillation achieves 96% volume utilization on 3D bin packing, while PPO plateaus around 90%. That 6 percentage point gap represents a meaningful improvement in packing efficiency.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: MCTS with mctx -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="mcts">Monte Carlo Tree Search with mctx</h2>

        <p>
            We use <a href="https://github.com/google-deepmind/mctx" target="_blank">mctx</a>, DeepMind's JAX-native MCTS library. The library is worth studying because it implements batched MCTS that compiles entirely under XLA—the entire search runs
            as a single compiled program, not Python loops calling into JAX. This is what makes it fast enough to compete with model-free methods on wall-clock time.
        </p>

        <p>
            The mctx library requires you to provide a "recurrent function" that simulates environment transitions. For problems with known dynamics, this is trivial—we just call the environment's step function:
        </p>

        <pre><code><span class="keyword">import</span> mctx

<span class="keyword">def</span> <span class="function">recurrent_fn</span>(model, rng_key, action, state):
    <span class="string">"""Environment model for MCTS simulation."""</span>
    model_params, model_state = model
    
    <span class="comment"># Step the environment (we have perfect dynamics!)</span>
    action_pair = unflatten_action(action)
    next_state, timestep = jax.vmap(env.step)(state, action_pair)
    
    <span class="comment"># Get network predictions for the next state</span>
    observation = timestep.observation
    (logits, value), _ = forward.apply(
        model_params, model_state, observation, is_eval=<span class="keyword">True</span>
    )
    
    <span class="comment"># Mask invalid actions</span>
    valid_mask = get_valid_action_mask(observation.action_mask)
    logits = apply_action_mask(logits, valid_mask)
    
    <span class="keyword">return</span> mctx.RecurrentFnOutput(
        reward=timestep.reward,
        discount=timestep.discount,
        prior_logits=logits,
        value=value,
    ), next_state</code></pre>

        <p>
            With this function defined, running MCTS is straightforward. We use <code class="inline-code">gumbel_muzero_policy</code>, which provides a more principled approach to exploration than vanilla MCTS (more on this shortly):
        </p>

        <pre><code><span class="comment"># Run MCTS from the current state</span>
policy_output = mctx.gumbel_muzero_policy(
    params=model,
    rng_key=key,
    root=mctx.RootFnOutput(
        prior_logits=network_logits,
        value=network_value,
        embedding=current_state,  <span class="comment"># The state IS the embedding</span>
    ),
    recurrent_fn=recurrent_fn,
    num_simulations=<span class="number">32</span>,          <span class="comment"># Search budget per decision</span>
    invalid_actions=~valid_mask,
)

<span class="comment"># Extract the improved policy (this is our training target)</span>
mcts_policy = policy_output.action_weights  <span class="comment"># Shape: (batch, num_actions)</span></code></pre>

        <p>
            The <code class="inline-code">action_weights</code> output is the probability distribution over actions that we use as our training target. This is computed from the search tree—in Gumbel MuZero specifically, it's a softmax over the prior
            logits plus the completed Q-values, which provides a principled policy improvement guarantee.
        </p>

        <h3>Under the Hood: How mctx Implements Batched Search</h3>

        <p>
            Understanding how mctx works is instructive if you've ever tried to write MCTS and gotten stuck with Python control flow. The core insight is that the entire search loop is implemented using <code class="inline-code">jax.lax.fori_loop</code>            and <code class="inline-code">jax.lax.while_loop</code>, which compile to XLA control flow rather than Python loops. Here's the main search loop:
        </p>

        <pre><code><span class="comment"># From mctx/_src/search.py</span>
<span class="keyword">def</span> <span class="function">body_fun</span>(sim, loop_state):
    rng_key, tree = loop_state
    rng_key, simulate_key, expand_key = jax.random.split(rng_key, <span class="number">3</span>)

    <span class="comment"># Simulate: walk down the tree selecting actions</span>
    parent_index, action = simulate(
        simulate_keys, tree, action_selection_fn, max_depth)

    <span class="comment"># Check if we've reached an unexpanded node</span>
    next_node_index = tree.children_index[batch_range, parent_index, action]
    next_node_index = jnp.where(next_node_index == Tree.UNVISITED,
                                sim + <span class="number">1</span>, next_node_index)

    <span class="comment"># Expand: add new node to the tree</span>
    tree = expand(params, expand_key, tree, recurrent_fn, 
                  parent_index, action, next_node_index)
    
    <span class="comment"># Backup: propagate values up the tree</span>
    tree = backward(tree, next_node_index)
    <span class="keyword">return</span> rng_key, tree</code></pre>

        <p>
            The simulation phase uses a while loop to walk down the existing tree until it finds an unvisited edge. At expansion, mctx calls your recurrent function to get the value and prior for the new node. The backup phase then propagates the leaf value up to
            the root using an incremental mean update:
        </p>

        <div class="math-block">
            \[ Q_{\text{parent}} \leftarrow \frac{N_{\text{parent}} \cdot Q_{\text{parent}} + G}{N_{\text{parent}} + 1} \quad\text{where}\quad G = r + \gamma \cdot V_{\text{leaf}} \]
        </div>

        <p>
            Because all of this is expressed as JAX control flow, the entire search compiles to a single XLA program. Combined with <code class="inline-code">vmap</code> over the batch dimension, this makes it feasible to run dozens of simulations per
            decision at scale.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Gumbel Exploration -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="gumbel">Gumbel Exploration</h2>

        <p>
            We use Gumbel MuZero rather than vanilla MCTS for a specific reason: when you have many actions and only a small simulation budget, traditional AlphaZero-style exploration can fail to visit enough actions to guarantee improvement. Gumbel MuZero, introduced
            in <a href="https://openreview.net/forum?id=bERaNdoegnO" target="_blank">"Policy Improvement by Planning with Gumbel"</a> (ICLR 2022), fixes this by sampling actions without replacement in a principled way.
        </p>

        <p>
            The foundation is the Gumbel-Max trick for sampling from a categorical distribution. If your policy network outputs logits \(\ell(a)\), you can sample from \(\pi(a) = \text{softmax}(\ell)\) by adding Gumbel noise and taking an argmax:
        </p>

        <div class="math-block">
            \[ g(a) \sim \text{Gumbel}(0,1), \qquad A = \arg\max_a \left[g(a) + \ell(a)\right] \]
        </div>

        <p>
            The paper extends this to the <strong>Gumbel-Top-k</strong> trick, which samples k actions without replacement. At the root node, Gumbel MuZero uses this to select a subset of candidate actions, then allocates the simulation budget across
            those actions using <strong>Sequential Halving</strong>—a bandit algorithm optimized for simple regret rather than cumulative regret. This is the right objective when you only care about finding the best action, not about the path you took
            to find it.
        </p>

        <p>
            You can see the mechanics directly in the mctx code:
        </p>

        <pre><code><span class="comment"># From mctx/_src/policies.py (gumbel_muzero_policy)</span>

<span class="comment"># 1) Mask invalid actions at the root</span>
root = root.replace(
    prior_logits=_mask_invalid_actions(root.prior_logits, invalid_actions)
)

<span class="comment"># 2) Sample Gumbel noise (same shape as logits)</span>
rng_key, gumbel_rng = jax.random.split(rng_key)
gumbel = gumbel_scale * jax.random.gumbel(
    gumbel_rng, shape=root.prior_logits.shape, dtype=root.prior_logits.dtype
)

<span class="comment"># 3) Run batched tree search with Gumbel-aware action selection</span>
search_tree = search.search(
    params=params,
    rng_key=rng_key,
    root=root,
    recurrent_fn=recurrent_fn,
    root_action_selection_fn=functools.partial(
        action_selection.gumbel_muzero_root_action_selection,
        num_simulations=num_simulations,
        max_num_considered_actions=max_num_considered_actions,
        qtransform=qtransform,
    ),
    interior_action_selection_fn=functools.partial(
        action_selection.gumbel_muzero_interior_action_selection,
        qtransform=qtransform,
    ),
    num_simulations=num_simulations,
    invalid_actions=invalid_actions,
    extra_data=action_selection.GumbelMuZeroExtraData(root_gumbel=gumbel),
)</code></pre>

        <p>
            One important detail that might surprise you: in Gumbel MuZero, the returned <code class="inline-code">action_weights</code> are <em>not</em> computed from visit counts like in AlphaZero. Instead, they're <code class="inline-code">softmax(prior_logits + completed_qvalues)</code>—a
            direct implementation of the policy improvement operator from the paper. This theoretical grounding is part of why Gumbel MuZero works well with small simulation budgets.
        </p>

        <h3>Gumbel MuZero vs Traditional MCTS</h3>

        <p>
            This distinction matters for understanding what we're actually training on:
        </p>

        <table>
            <tr>
                <th>Aspect</th>
                <th>Traditional MCTS (AlphaZero)</th>
                <th>Gumbel MuZero</th>
            </tr>
            <tr>
                <td>Policy target</td>
                <td><code class="inline-code">n(s,a) / n(s)</code> (visit counts)</td>
                <td><code class="inline-code">softmax(prior + Q)</code> (policy improvement)</td>
            </tr>
            <tr>
                <td>Root exploration</td>
                <td>UCT + Dirichlet noise</td>
                <td>Gumbel-Top-k + Sequential Halving</td>
            </tr>
            <tr>
                <td>Theoretical guarantee</td>
                <td>Asymptotic (infinite simulations)</td>
                <td>Finite-sample policy improvement</td>
            </tr>
        </table>

        <p>
            The original <a href="https://arxiv.org/abs/1705.08439" target="_blank">Expert Iteration paper</a> uses visit count proportions as targets—the idea being that MCTS visits better actions more often. Gumbel MuZero instead computes targets by
            <em>directly applying a policy improvement operator</em>: it adds the completed Q-values to the prior logits and takes a softmax. This provides stronger guarantees when simulation budgets are small relative to the action space.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: The Problem -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="problem">The Problem: 3D Bin Packing</h2>

        <p>
            What makes 3D bin packing particularly interesting from a reinforcement learning perspective is that it's a perfect-information, deterministic, single-agent planning task. Once you sample a problem instance (a set of items to pack), the environment dynamics
            are completely known. You place an item, the container state updates deterministically, and you get a reward proportional to the volume you just filled. This structure turns out to be ideal for tree search methods—something we exploit heavily.
        </p>

        <p>
            Let's formally specify the MDP we're solving. BinPack is an episodic MDP with deterministic dynamics, a large discrete action space, and rewards that are exactly computable from the environment state:
        </p>

        <div class="math-block">
            \[ \text{State } s_t = (\text{EMS list},\, \text{items remaining},\, \text{placed items},\, \text{container occupancy}) \] \[ \text{Action } a_t = (\text{ems\_id},\, \text{item\_id}) \in \{0..E-1\}\times\{0..I-1\} \] \[ s_{t+1} = f(s_t, a_t)\quad \text{(exact
            transition from env.step)} \] \[ r_t = \Delta \text{utilization} \in [0,1] \]
        </div>

        <p>
            The crucial observation here is that we have access to the exact transition function \(f\). Unlike Atari games where we'd need to learn a world model, or robotics where dynamics are noisy and partially observable, bin packing gives us perfect simulation
            for free. This means MCTS can roll forward with ground-truth dynamics, and the neural network's job is simply to imitate the search-improved policy. That asymmetry—cheap perfect simulation combined with expensive search—is exactly where search-based
            policy iteration shines.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: The Jumanji Environment -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="jumanji">The Jumanji Environment</h2>

        <p>
            We use InstaDeep's <a href="https://github.com/instadeepai/jumanji" target="_blank">Jumanji</a> library, which provides a JAX-native BinPack environment. The key abstraction that makes this environment tractable is the concept of <strong>Empty Maximal Spaces (EMS)</strong>—rectangular
            regions inside the container where items can potentially be placed.
        </p>

        <p>
            At each step, the agent makes a joint decision: which EMS to place an item in (from up to 40 candidates), and which item to place (from up to 20 items). This gives a joint action space of 40 × 20 = 800 discrete actions. The environment uses dense rewards,
            meaning each placement adds the item's volume (normalized by container volume) to the cumulative return. A perfect packing that uses all available space yields a return of 1.0.
        </p>

        <pre><code><span class="comment"># Environment setup</span>
<span class="keyword">import</span> jumanji

env = jumanji.make(<span class="string">"BinPack-v2"</span>)

<span class="comment"># Action space dimensions</span>
obs_num_ems = <span class="number">40</span>   <span class="comment"># Observable empty maximal spaces</span>
max_num_items = <span class="number">20</span>  <span class="comment"># Maximum items per episode</span>
num_actions = obs_num_ems * max_num_items  <span class="comment"># 800 total</span>

<span class="comment"># We flatten the action for simpler MCTS handling</span>
<span class="keyword">def</span> <span class="function">unflatten_action</span>(action):
    <span class="string">"""Flat index → (ems_id, item_id)"""</span>
    ems_id = action // max_num_items
    item_id = action % max_num_items
    <span class="keyword">return</span> jnp.stack([ems_id, item_id], axis=-<span class="number">1</span>)</code></pre>

        <p>
            The environment's observation structure is designed to be "planning-friendly"—rather than giving raw pixels or low-level state, it provides two sets of tokens (EMS and items) plus boolean masks defining what's valid. This aligns nicely with transformer-style
            encoders and with our flattened (E×I) action head. Each EMS is represented by its 6D bounding box coordinates (x1, x2, y1, y2, z1, z2), and each item by its three dimensions (x_len, y_len, z_len). The critical piece is the
            <code class="inline-code">action_mask</code>: a boolean matrix where entry (e, i) is True if item i can legally be placed in EMS e.
        </p>

        <p>
            How does the environment compute this joint action mask? For each EMS and each item, it tests whether the item fits in that EMS and whether the item is still available. This is implemented with nested <code class="inline-code">jax.vmap</code>,
            so the entire mask computation happens in parallel:
        </p>

        <pre><code><span class="comment"># From jumanji/environments/packing/bin_pack/env.py</span>
<span class="keyword">def</span> <span class="function">is_action_allowed</span>(ems, ems_mask, item, item_mask, item_placed):
    item_fits_in_ems = item_fits_in_item(item, item_from_space(ems))
    <span class="keyword">return</span> ~item_placed & item_mask & ems_mask & item_fits_in_ems

action_mask = jax.vmap(
    jax.vmap(is_action_allowed, in_axes=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)),
    in_axes=(<span class="number">0</span>, <span class="number">0</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>),
)(obs_ems, obs_ems_mask, items, items_mask, items_placed)</code></pre>

        <p>
            One subtle detail: the full environment can track more than 40 EMS internally, but the observation only returns the <code class="inline-code">obs_num_ems</code> largest by volume. This keeps the observation size fixed, turning a variable-structure
            geometric process into a fixed-shape tensor problem—exactly what we need for efficient JAX/XLA compilation. The environment is also strict about invalid actions: if you choose an action where the item doesn't fit or has already been placed,
            the episode terminates immediately. This makes action masking a first-class concern, both in PPO and especially in MCTS where we want zero probability mass on invalid moves.
        </p>

        <p>
            The EMS representation isn't unique to Jumanji—it's a classic way to represent free space as a non-disjoint set of maximal empty cuboids. If you're interested in the "pre-RL" perspective on this representation, Parreño et al.'s <a href="https://www.uv.es/sestio/TechRep/tr03-07.pdf"
                target="_blank">technical report</a> on GRASP heuristics and Zhao et al.'s <a href="https://eprints.soton.ac.uk/364226/1/A_20Comparative_20Review_20of_203D_20Container_20Loading_20Algorithms.pdf" target="_blank">comparative review</a> of 3D
            container loading provide excellent background. The key insight is that EMS makes the problem look like "choose a space + choose an item", which is exactly how our policy and value networks will model it.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: JAX Parallelism -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="jax">JAX Parallelism</h2>

        <p>
            The historical challenge with search-based policy iteration is computational cost. Running 32 MCTS simulations per action adds significant overhead compared to a single network forward pass. But JAX's parallelization primitives change the calculus dramatically.
        </p>

        <p>
            The key primitives are <code class="inline-code">jax.vmap</code> for vectorizing over the batch dimension and <code class="inline-code">jax.pmap</code> for distributing across devices. With <code class="inline-code">vmap</code>, a single line
            of code turns a function that processes one environment into a function that processes thousands in parallel:
        </p>

        <pre><code><span class="comment"># Without vmap: process episodes one at a time</span>
<span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):
    state, timestep = env.step(states[i], actions[i])

<span class="comment"># With vmap: all episodes processed in parallel</span>
state, timestep = jax.vmap(env.step)(states, actions)</code></pre>

        <p>
            The <code class="inline-code">pmap</code> primitive replicates computation across all available GPUs or TPUs. Each device processes a shard of the batch independently, and gradients are synchronized using <code class="inline-code">jax.lax.pmean</code>:
        </p>

        <pre><code><span class="decorator">@partial</span>(jax.pmap, axis_name=<span class="string">"devices"</span>)
<span class="keyword">def</span> <span class="function">train_step</span>(model, opt_state, batch):
    <span class="comment"># Compute gradients on this device's shard</span>
    grads = jax.grad(loss_fn)(model, batch)
    
    <span class="comment"># Synchronize gradients across all devices</span>
    grads = jax.lax.pmean(grads, axis_name=<span class="string">"devices"</span>)
    
    <span class="comment"># Apply optimizer update</span>
    updates, opt_state = optimizer.update(grads, opt_state)
    model = optax.apply_updates(model, updates)
    <span class="keyword">return</span> model, opt_state</code></pre>

        <p>
            Together, vmap and pmap give us massive throughput. On 4 GPUs with batch size 1024, we process roughly 20,000 MCTS-guided decisions per second—competitive with PPO's sample efficiency despite running 32 simulations per decision. The JAX documentation
            on <a href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html" target="_blank">parallelism</a> provides an excellent introduction to these concepts.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Neural Network Architecture -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="architecture">Neural Network Architecture</h2>

        <p>
            We adapt Jumanji's <a href="https://github.com/instadeepai/jumanji/blob/main/jumanji/training/networks/bin_pack/actor_critic.py" target="_blank">A2C architecture</a> for our policy-value network. The key insight is using <strong>cross-attention</strong>            between EMS tokens and item tokens. This lets the network reason about which items fit in which spaces, with the action mask gating the attention to only consider valid placement combinations.
        </p>

        <p>
            The architecture processes EMS and item tokens through alternating layers of self-attention (within each token type) and cross-attention (between types). After encoding, the policy head computes logits via a bilinear form—<code class="inline-code">logits[e,i] = ems_h[e] · items_h[i]</code>—which
            naturally produces the (E × I) shaped output we need. The value head pools the embeddings and predicts expected utilization in [0, 1].
        </p>

        <pre><code><span class="comment"># Architecture overview</span>
<span class="comment">#</span>
<span class="comment">#   ┌─────────────┐     ┌─────────────┐</span>
<span class="comment">#   │  EMS tokens │     │ Item tokens │</span>
<span class="comment">#   │ (40 × 6D)   │     │ (20 × 3D)   │</span>
<span class="comment">#   └──────┬──────┘     └──────┬──────┘</span>
<span class="comment">#          │                   │</span>
<span class="comment">#          ▼                   ▼</span>
<span class="comment">#   ┌─────────────┐     ┌─────────────┐</span>
<span class="comment">#   │ Self-Attn   │     │ Self-Attn   │</span>
<span class="comment">#   └──────┬──────┘     └──────┬──────┘</span>
<span class="comment">#          │                   │</span>
<span class="comment">#          └────────┬──────────┘</span>
<span class="comment">#                   ▼</span>
<span class="comment">#          ┌───────────────┐</span>
<span class="comment">#          │ Cross-Attn    │  ← EMS ↔ Items interaction</span>
<span class="comment">#          │ (bidirectional)│    (gated by action_mask)</span>
<span class="comment">#          └───────┬───────┘</span>
<span class="comment">#                  │</span>
<span class="comment">#          ┌───────┴───────┐</span>
<span class="comment">#          ▼               ▼</span>
<span class="comment">#   ┌─────────────┐  ┌─────────────┐</span>
<span class="comment">#   │ Policy Head │  │ Value Head  │</span>
<span class="comment">#   │ (E×I logits)│  │ (scalar)    │</span>
<span class="comment">#   └─────────────┘  └─────────────┘</span></code></pre>

        <p>
            We use <a href="https://dm-haiku.readthedocs.io/" target="_blank">Haiku</a> to build the network. Haiku's <code class="inline-code">hk.transform_with_state</code> pattern cleanly separates the stateful module definition from the pure functional
            interface that JAX requires:
        </p>

        <pre><code><span class="keyword">import</span> haiku <span class="keyword">as</span> hk

<span class="keyword">def</span> <span class="function">forward_fn</span>(observation, is_eval=<span class="keyword">False</span>):
    net = BinPackPolicyValueNet(
        num_transformer_layers=<span class="number">4</span>,
        transformer_num_heads=<span class="number">4</span>,
        transformer_key_size=<span class="number">32</span>,
        transformer_mlp_units=(<span class="number">256</span>, <span class="number">256</span>),
    )
    <span class="keyword">return</span> net(observation, is_training=<span class="keyword">not</span> is_eval)

<span class="comment"># Transform to pure functions</span>
forward = hk.without_apply_rng(hk.transform_with_state(forward_fn))

<span class="comment"># Initialize parameters</span>
params, state = forward.init(rng_key, dummy_obs)

<span class="comment"># Apply is now a pure function</span>
(logits, value), new_state = forward.apply(params, state, observation)</code></pre>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Experiments -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="experiments">Experiments</h2>

        <p>
            We trained both search distillation and PPO on BinPack-v2 for 800 iterations across 5 random seeds. To ensure a fair comparison, both methods use the same network architecture, batch sizes, and evaluation protocol. The key hyperparameters are:
        </p>

        <table>
            <tr>
                <th>Hyperparameter</th>
                <th>Search Distillation</th>
                <th>PPO</th>
            </tr>
            <tr>
                <td>Rollout batch size</td>
                <td>1024</td>
                <td>1024</td>
            </tr>
            <tr>
                <td>Training batch size</td>
                <td>4096</td>
                <td>4096</td>
            </tr>
            <tr>
                <td>MCTS simulations</td>
                <td>32</td>
                <td>—</td>
            </tr>
            <tr>
                <td>PPO epochs per iteration</td>
                <td>—</td>
                <td>4</td>
            </tr>
            <tr>
                <td>Learning rate</td>
                <td>1e-3</td>
                <td>3e-4</td>
            </tr>
            <tr>
                <td>Transformer layers</td>
                <td>4</td>
                <td>4</td>
            </tr>
        </table>

        <figure>
            <img src="ppo_vs_distill.gif" alt="PPO vs Search Distillation learning curves">
            <figcaption>
                <strong>Figure 1.</strong> Learning curves comparing search distillation (light blue) vs PPO (dark blue) on BinPack-v2. Shaded regions show ±1 standard error across 5 seeds. Search distillation converges to 96% volume utilization while
                PPO plateaus around 90%.
            </figcaption>
        </figure>

        <p>
            The results are clear: search distillation achieves 96% volume utilization compared to PPO's 90%, a 6 percentage point improvement that translates to significantly better packing efficiency in practice. Despite running 32 MCTS simulations per action,
            JAX parallelism keeps training time competitive—both methods reach convergence in roughly the same wall-clock time.
        </p>

        <p>
            The learning curves also show that search distillation is more stable. PPO exhibits the characteristic variance of policy gradient methods, with performance fluctuating throughout training. Search distillation's learning curve is smoother, reflecting
            the richer supervision signal from the search action distributions.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Implementation Details -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="implementation">Implementation Details</h2>

        <p>
            A few implementation details are worth highlighting, as they address common pitfalls when working with JAX and MCTS.
        </p>

        <h3>Using jax.lax.scan for Efficient Loops</h3>

        <p>
            Python loops inside JIT-compiled functions are unrolled at compile time, causing slow compilation and memory issues for long episodes. The solution is <code class="inline-code">jax.lax.scan</code>, which compiles the loop body once and executes
            it repeatedly:
        </p>

        <pre><code><span class="comment"># Inefficient: Python loop gets unrolled</span>
<span class="keyword">def</span> <span class="function">rollout_bad</span>(state, keys):
    trajectory = []
    <span class="keyword">for</span> key <span class="keyword">in</span> keys:
        state, data = step(state, key)
        trajectory.append(data)
    <span class="keyword">return</span> trajectory

<span class="comment"># Efficient: scan compiles once, runs fast</span>
<span class="keyword">def</span> <span class="function">rollout_good</span>(state, keys):
    <span class="keyword">def</span> <span class="function">step_fn</span>(carry, key):
        state = carry
        state, data = step(state, key)
        <span class="keyword">return</span> state, data
    
    final_state, trajectory = jax.lax.scan(step_fn, state, keys)
    <span class="keyword">return</span> trajectory</code></pre>

        <h3>Action Masking Without NaNs</h3>

        <p>
            Invalid actions must be masked before computing the softmax for action selection. A common mistake is using <code class="inline-code">-inf</code> for invalid logits, which causes NaN when all actions are invalid (as can happen at episode termination).
            The solution is to use a large but finite negative value:
        </p>

        <pre><code><span class="keyword">def</span> <span class="function">apply_action_mask</span>(logits, valid_mask):
    <span class="string">"""Set invalid action logits to a large negative value."""</span>
    <span class="comment"># Center for numerical stability</span>
    logits = logits - jnp.max(logits, axis=-<span class="number">1</span>, keepdims=<span class="keyword">True</span>)
    <span class="comment"># Use finite minimum (not -inf) to avoid NaN</span>
    <span class="keyword">return</span> jnp.where(valid_mask, logits, jnp.finfo(logits.dtype).min)</code></pre>

        <p>
            The mctx library uses the same approach for the same reason—at the end of an episode, all actions can become invalid, and the code needs to handle this gracefully.
        </p>

        <h3>When to Use Search Distillation</h3>

        <p>
            Search-based policy iteration is particularly well-suited to problems where you have exact dynamics (so MCTS can simulate accurately), episodes are relatively short (so MCTS overhead doesn't compound excessively), actions have complex dependencies that
            gradient signals might miss, and you have parallel compute to amortize the cost of search. Bin packing hits all four criteria.
        </p>

        <p>
            PPO remains preferable when dynamics must be learned rather than simulated, horizons are very long (hundreds of steps), action spaces are simple with clear reward gradients, or when you need the simplicity of a model-free approach.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: References -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="refs">References</h2>

        <h3>Code</h3>

        <p>
            The complete implementation is available on <a href="https://github.com/Aneeshers/expert-iteration-rl" target="_blank">GitHub</a>. Key dependencies include <a href="https://github.com/google-deepmind/mctx" target="_blank">mctx</a> for MCTS,
            <a href="https://github.com/instadeepai/jumanji" target="_blank">Jumanji</a> for the BinPack environment, <a href="https://dm-haiku.readthedocs.io/" target="_blank">Haiku</a> for neural networks, and <a href="https://optax.readthedocs.io/"
                target="_blank">Optax</a> for optimization. The <a href="https://github.com/sotetsuk/pgx/tree/main/examples/alphazero" target="_blank">PGX AlphaZero</a> example was a helpful reference for JAX-native AlphaZero patterns.
        </p>

        <h3>Papers</h3>

        <ol class="references">
            <li>
                Anthony et al., <a href="https://arxiv.org/abs/1705.08439" target="_blank">"Thinking Fast and Slow with Deep Learning and Tree Search"</a>, NeurIPS 2017. The original Expert Iteration paper—closely related to our approach.
            </li>
            <li>
                Danihelka et al., <a href="https://openreview.net/forum?id=bERaNdoegnO" target="_blank">"Policy Improvement by Planning with Gumbel"</a>, ICLR 2022. The theoretical foundation for Gumbel MuZero and our policy improvement operator.
            </li>
            <li>
                Grill et al., <a href="https://arxiv.org/abs/2007.12509" target="_blank">"Monte-Carlo Tree Search as Regularized Policy Optimization"</a>, ICML 2020. Interprets MCTS as approximately solving a regularized policy optimization problem.
            </li>
            <li>
                Silver et al., <a href="https://www.nature.com/articles/nature24270" target="_blank">"Mastering the game of Go without human knowledge"</a>, Nature 2017. The AlphaZero paper.
            </li>
            <li>
                Schrittwieser et al., <a href="https://arxiv.org/abs/1911.08265" target="_blank">"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"</a>, Nature 2020. The MuZero paper.
            </li>
            <li>
                Schulman et al., <a href="https://arxiv.org/abs/1707.06347" target="_blank">"Proximal Policy Optimization Algorithms"</a>, arXiv 2017. The PPO paper.
            </li>
            <li>
                Bonnet et al., <a href="https://arxiv.org/abs/2306.09884" target="_blank">"Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX"</a>, 2023. The Jumanji environment suite including BinPack.
            </li>
            <li>
                Karnin, Koren, Somekh, <a href="https://proceedings.mlr.press/v28/karnin13.pdf" target="_blank">"Almost Optimal Exploration in Multi-Armed Bandits"</a>, ICML 2013. The Sequential Halving algorithm used in Gumbel MuZero.
            </li>
        </ol>

        <h2>Citation</h2>

        <pre><code>@misc{muppidi2026searchdistill,
  title={Policy Iteration via Search Distillation for 3D Bin Packing},
  author={Muppidi, Aneesh},
  year={2026},
  url={https://github.com/Aneeshers/expert-iteration-rl}
}</code></pre>

        <footer>
            <p>
                <a href="https://aneeshers.github.io"><strong>Aneesh Muppidi</strong></a><br> February 2026
            </p>
            <p style="margin-top: 16px;">
                <a href="https://github.com/Aneeshers/expert-iteration-rl">GitHub</a> ·
                <a href="https://twitter.com/aneeshers">Twitter</a>
            </p>
        </footer>
    </div>
</body>

</html>