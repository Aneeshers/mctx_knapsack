<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Expert Iteration for 3D Bin Packing</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <style>
        @font-face {
            font-family: 'Berkeley Mono';
            src: url('BerkeleyMonoTrial-Regular.otf') format('opentype');
            font-weight: normal;
            font-style: normal;
        }
        
         :root {
            --bg: #ffffff;
            --fg: #1a1a1a;
            --accent: #2563eb;
            --accent-light: #dbeafe;
            --code-bg: #f8f9fa;
            --border: #e5e7eb;
            --muted: #6b7280;
            --success: #059669;
            --warn: #b45309;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Berkeley Mono', 'SF Mono', 'Consolas', monospace;
            background: var(--bg);
            color: var(--fg);
            line-height: 1.75;
            font-size: 14px;
        }
        
        .container {
            max-width: 860px;
            margin: 0 auto;
            padding: 60px 24px;
        }
        
        header {
            margin-bottom: 48px;
            border-bottom: 1px solid var(--border);
            padding-bottom: 32px;
        }
        
        .tag {
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: var(--accent);
            margin-bottom: 16px;
        }
        
        h1 {
            font-size: 28px;
            font-weight: 600;
            line-height: 1.3;
            margin-bottom: 20px;
            letter-spacing: -0.5px;
        }
        
        .subtitle {
            font-size: 15px;
            color: var(--muted);
            margin-bottom: 24px;
            line-height: 1.6;
        }
        
        .meta {
            font-size: 12px;
            color: var(--muted);
            margin-bottom: 24px;
        }
        
        .meta a {
            color: var(--fg);
            text-decoration: none;
        }
        
        .meta a:hover {
            color: var(--accent);
        }
        
        .buttons {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
        }
        
        .btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 10px 16px;
            font-family: inherit;
            font-size: 12px;
            text-decoration: none;
            border: 1px solid var(--border);
            background: white;
            color: var(--fg);
            transition: all 0.15s ease;
        }
        
        .btn:hover {
            border-color: var(--accent);
            color: var(--accent);
        }
        
        .btn-primary {
            background: var(--fg);
            color: white;
            border-color: var(--fg);
        }
        
        .btn-primary:hover {
            background: var(--accent);
            border-color: var(--accent);
            color: white;
        }
        
        .visual-abstract {
            margin: 48px 0;
            text-align: center;
        }
        
        .visual-abstract img {
            max-width: 100%;
            border: 1px solid var(--border);
        }
        
        .visual-abstract figcaption {
            font-size: 11px;
            color: var(--muted);
            margin-top: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .tldr {
            background: var(--code-bg);
            padding: 20px 24px;
            margin: 32px 0;
            border: 1px solid var(--border);
        }
        
        .tldr-label {
            font-size: 10px;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: var(--accent);
            margin-bottom: 8px;
            font-weight: 600;
        }
        
        .tldr p {
            margin: 0;
            font-size: 13px;
        }
        
        .toc {
            background: white;
            padding: 18px 22px;
            margin: 24px 0 40px 0;
            border: 1px solid var(--border);
        }
        
        .toc .toc-title {
            font-size: 10px;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: var(--muted);
            margin-bottom: 10px;
            font-weight: 600;
        }
        
        .toc ul {
            margin: 0 0 0 18px;
        }
        
        .toc li {
            margin: 6px 0;
        }
        
        .insight-box {
            border: 2px solid var(--accent);
            padding: 24px;
            margin: 40px 0;
            background: var(--accent-light);
            font-size: 13px;
        }
        
        .insight-box pre {
            background: white;
            border: 1px solid var(--border);
            padding: 16px;
            margin: 16px 0 0 0;
            font-size: 11px;
            line-height: 1.5;
            white-space: pre;
            overflow-x: auto;
            color: var(--fg);
        }
        
        .callout {
            border: 1px solid var(--border);
            background: white;
            padding: 18px 20px;
            margin: 28px 0;
        }
        
        .callout h4 {
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--accent);
            margin-bottom: 8px;
        }
        
        .callout.warn h4 {
            color: var(--warn);
        }
        
        .callout.warn {
            border-left: 3px solid var(--warn);
        }
        
        h2 {
            font-size: 18px;
            font-weight: 600;
            margin: 48px 0 20px 0;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border);
        }
        
        h3 {
            font-size: 14px;
            font-weight: 600;
            margin: 32px 0 12px 0;
            color: var(--accent);
        }
        
        h4 {
            font-size: 13px;
            font-weight: 600;
            margin: 18px 0 10px 0;
        }
        
        p {
            margin-bottom: 16px;
        }
        
        pre {
            background: #1a1a1a;
            color: #e0e0e0;
            padding: 20px;
            overflow-x: auto;
            margin: 24px 0;
            font-size: 12px;
            line-height: 1.6;
            border-radius: 0;
        }
        
        code {
            font-family: 'Berkeley Mono', monospace;
        }
        
        .inline-code {
            background: var(--code-bg);
            padding: 2px 6px;
            font-size: 12px;
            border: 1px solid var(--border);
        }
        
        .keyword {
            color: #569cd6;
        }
        
        .function {
            color: #dcdcaa;
        }
        
        .string {
            color: #ce9178;
        }
        
        .comment {
            color: #6a9955;
        }
        
        .number {
            color: #b5cea8;
        }
        
        .decorator {
            color: #c586c0;
        }
        
        figure {
            margin: 40px 0;
        }
        
        figure img {
            width: 100%;
            border: 1px solid var(--border);
            background: white;
        }
        
        figcaption {
            font-size: 12px;
            color: var(--muted);
            margin-top: 12px;
            padding: 0 8px;
            line-height: 1.6;
        }
        
        figcaption strong {
            color: var(--fg);
        }
        
        .math-block {
            background: var(--code-bg);
            padding: 20px;
            margin: 24px 0;
            text-align: center;
            border: 1px solid var(--border);
            overflow-x: auto;
        }
        
        ul,
        ol {
            margin: 16px 0 16px 24px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            font-size: 12px;
        }
        
        th,
        td {
            border: 1px solid var(--border);
            padding: 10px 12px;
            text-align: left;
        }
        
        th {
            background: var(--code-bg);
            font-weight: 600;
        }
        
        .highlight-box {
            border-left: 3px solid var(--accent);
            padding: 16px 20px;
            margin: 32px 0;
            background: var(--code-bg);
        }
        
        .highlight-box h4 {
            color: var(--accent);
            margin-bottom: 8px;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 32px 0;
        }
        
        .comparison-item {
            border: 1px solid var(--border);
            padding: 20px;
            background: white;
        }
        
        .comparison-item h4 {
            font-size: 13px;
            margin-bottom: 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border);
        }
        
        .comparison-item.winner {
            border-color: var(--success);
            background: #ecfdf5;
        }
        
        .comparison-item.winner h4 {
            color: var(--success);
        }
        
        @media (max-width: 600px) {
            .comparison {
                grid-template-columns: 1fr;
            }
        }
        
        .findings {
            display: grid;
            gap: 16px;
            margin: 24px 0;
        }
        
        .finding {
            border-left: 3px solid var(--accent);
            padding: 12px 16px;
            background: var(--code-bg);
        }
        
        .finding-num {
            font-size: 10px;
            color: var(--accent);
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        a {
            color: var(--accent);
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        .note {
            font-size: 11px;
            color: var(--muted);
            font-style: italic;
        }
        
        .source-note {
            font-size: 11px;
            color: var(--muted);
            margin-top: -10px;
        }
        
        .references {
            font-size: 12px;
            line-height: 1.8;
        }
        
        .references li {
            margin-bottom: 8px;
            padding-left: 8px;
        }
        
        footer {
            margin-top: 64px;
            padding-top: 32px;
            border-top: 1px solid var(--border);
            font-size: 12px;
            color: var(--muted);
        }
        
        .algorithm {
            border: 1px solid var(--border);
            margin: 32px 0;
            background: white;
        }
        
        .algorithm-header {
            background: var(--code-bg);
            padding: 12px 16px;
            font-size: 12px;
            font-weight: 600;
            border-bottom: 1px solid var(--border);
        }
        
        .algorithm-body {
            padding: 16px;
            font-size: 12px;
        }
        
        .algorithm-body .line {
            margin: 4px 0;
            padding-left: 24px;
            position: relative;
        }
        
        .algorithm-body .line-num {
            position: absolute;
            left: 0;
            color: var(--muted);
            font-size: 10px;
        }
    </style>
</head>

<body>
    <div class="container">
        <header>
            <div class="tag">Reinforcement Learning · JAX · Combinatorial Optimization</div>
            <h1>Expert Iteration for 3D Bin Packing</h1>
            <p class="subtitle">
                AlphaZero without self-play! For deterministic single-agent tasks where transitions are known, we can treat MCTS as a teacher that performs lookahead and produces improved policy targets. We run batched MCTS to generate action weights (and values), and
                distill them into a fast neural policy—using JAX/XLA to make search and training throughput competitive with model-free baselines like PPO.
            </p>
            <div class="meta">
                <a href="https://aneeshers.github.io">Aneesh Muppidi</a> · February 2026
            </div>
            <div class="buttons">
                <a href="https://github.com/Aneeshers/expert-iteration-rl" class="btn btn-primary" target="_blank">
                    GitHub
                </a>
                <a href="https://github.com/Aneeshers/expert-iteration-rl/blob/main/train_expert_iteration_binpack.py" class="btn" target="_blank">
                    Expert Iteration Code
                </a>
                <a href="https://github.com/Aneeshers/expert-iteration-rl/blob/main/train_ppo_binpack.py" class="btn" target="_blank">
                    PPO Baseline
                </a>
            </div>
        </header>

        <figure class="visual-abstract">
            <img src="alphazero_binpack.gif" alt="Expert Iteration for 3D Bin Packing">
            <figcaption>Left: The 3D bin packing problem. Fit as many items as possible into a fixed container to maximize volume utilization. Items are placed one at a time; opacity shows placement order (newer = more solid). Right: MCTS explores possible placements
                before each decision, with node size indicating visit count.

            </figcaption>
        </figure>

        <div class="tldr">
            <div class="tldr-label">TL;DR</div>
            <p>
                Expert Iteration combines MCTS with neural network distillation to solve planning problems. With JAX's <code class="inline-code">vmap</code> and <code class="inline-code">pmap</code>, we can run MCTS fast enough to compete with PPO on
                wall-clock time—while achieving <strong>96% volume utilization</strong> vs PPO's 90% on 3D bin packing.
            </p>
        </div>

        <div class="toc">
            <div class="toc-title">Contents</div>
            <ul>
                <li><a href="#problem">The Problem: 3D Bin Packing</a></li>
                <li><a href="#jumanji">The Jumanji Environment</a></li>
                <li><a href="#exit">Expert Iteration: Teaching a Network with Search</a></li>
                <li><a href="#mcts">Monte Carlo Tree Search with mctx</a></li>
                <li><a href="#gumbel">Gumbel Exploration</a></li>
                <li><a href="#jax">JAX Parallelism</a></li>
                <li><a href="#architecture">Neural Network Architecture</a></li>
                <li><a href="#experiments">Experiments</a></li>
                <li><a href="#implementation">Implementation Details</a></li>
                <li><a href="#refs">References</a></li>
            </ul>
        </div>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: The Problem -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="problem">The Problem: 3D Bin Packing</h2>

        <p>
            I first encountered bin packing in Harvard's <a href="https://docs.google.com/document/d/1zr1xozlsfoF0hhABF46xvXZhK62Jy18x/edit" target="_blank">CS 124</a> (Data Structures and Algorithms), where it appeared alongside other classic NP-hard
            problems. The setup is deceptively simple: you're given a set of rectangular boxes and a container, and your goal is to pack as many boxes as possible to maximize volume utilization. It sounds like something you could solve by just being clever
            about placement order, but the combinatorial explosion makes this problem far harder than it appears.
        </p>

        <p>
            In its 3D form, bin packing is equivalent to the <a href="https://en.wikipedia.org/wiki/Knapsack_problem" target="_blank">3D Knapsack Problem</a>—one of Karp's 21 NP-complete problems. The difficulty isn't just fitting boxes into the container;
            it's navigating the astronomical number of possible placements. With <em>n</em> items and <em>m</em> potential positions, the search space grows roughly as O(m<sup>n</sup>). Even for modest problem sizes, brute-force search is hopeless.
        </p>

        <p>
            The 1D bin packing problem is already NP-hard, as you can reduce the <a href="https://en.wikipedia.org/wiki/Partition_problem" target="_blank">PARTITION</a> problem to it. Adding two more dimensions only compounds the difficulty. There's no
            known polynomial-time algorithm that finds optimal solutions, which is precisely why heuristics and learning-based approaches have become so valuable for these problems. For a thorough treatment of computational complexity and bin packing
            specifically, Garey and Johnson's <a href="https://perso.limos.fr/~palafour/PAPERS/PDF/Garey-Johnson79.pdf" target="_blank"><em>Computers and Intractability</em></a> remains the canonical reference.
        </p>

        <p>
            What makes 3D bin packing particularly interesting from a reinforcement learning perspective is that it's a perfect-information, deterministic, single-agent planning task. Once you sample a problem instance (a set of items to pack), the environment dynamics
            are completely known. You place an item, the container state updates deterministically, and you get a reward proportional to the volume you just filled. This structure turns out to be ideal for tree search methods—something we'll exploit heavily.
        </p>

        <p>
            Before diving into the algorithm, let's formally specify the MDP we're solving. This framing will help explain why MCTS is so effective here. BinPack is an episodic MDP with deterministic dynamics, a large discrete action space, and rewards that are exactly
            computable from the environment state:
        </p>

        <div class="math-block">
            \[ \text{State } s_t = (\text{EMS list},\, \text{items remaining},\, \text{placed items},\, \text{container occupancy}) \] \[ \text{Action } a_t = (\text{ems\_id},\, \text{item\_id}) \in \{0..E-1\}\times\{0..I-1\} \] \[ s_{t+1} = f(s_t, a_t)\quad \text{(exact
            transition from env.step)} \] \[ r_t = \Delta \text{utilization} \in [0,1] \]
        </div>

        <p>
            The crucial observation here is that we have access to the exact transition function \(f\). Unlike Atari games where we'd need to learn a world model, or robotics where dynamics are noisy and partially observable, bin packing gives us perfect simulation
            for free. This means MCTS can roll forward with ground-truth dynamics, and the neural network's job is simply to imitate the search-improved policy. That asymmetry—cheap perfect simulation combined with expensive search—is exactly where Expert
            Iteration shines.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: The Jumanji Environment -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="jumanji">The Jumanji Environment</h2>

        <p>
            We use InstaDeep's <a href="https://github.com/instadeepai/jumanji" target="_blank">Jumanji</a> library, which provides a JAX-native BinPack environment. The key abstraction that makes this environment tractable is the concept of <strong>Empty Maximal Spaces (EMS)</strong>—rectangular
            regions inside the container where items can potentially be placed.
        </p>

        <p>
            At each step, the agent makes a joint decision: which EMS to place an item in (from up to 40 candidates), and which item to place (from up to 20 items). This gives a joint action space of 40 × 20 = 800 discrete actions. The environment uses dense rewards,
            meaning each placement adds the item's volume (normalized by container volume) to the cumulative return. A perfect packing that uses all available space yields a return of 1.0.
        </p>

        <pre><code><span class="comment"># Environment setup</span>
<span class="keyword">import</span> jumanji

env = jumanji.make(<span class="string">"BinPack-v2"</span>)

<span class="comment"># Action space dimensions</span>
obs_num_ems = <span class="number">40</span>   <span class="comment"># Observable empty maximal spaces</span>
max_num_items = <span class="number">20</span>  <span class="comment"># Maximum items per episode</span>
num_actions = obs_num_ems * max_num_items  <span class="comment"># 800 total</span>

<span class="comment"># We flatten the action for simpler MCTS handling</span>
<span class="keyword">def</span> <span class="function">unflatten_action</span>(action):
    <span class="string">"""Flat index → (ems_id, item_id)"""</span>
    ems_id = action // max_num_items
    item_id = action % max_num_items
    <span class="keyword">return</span> jnp.stack([ems_id, item_id], axis=-<span class="number">1</span>)</code></pre>

        <p>
            The environment's observation structure is designed to be "planning-friendly"—rather than giving raw pixels or low-level state, it provides two sets of tokens (EMS and items) plus boolean masks defining what's valid. This aligns nicely with transformer-style
            encoders and with our flattened (E×I) action head. Each EMS is represented by its 6D bounding box coordinates (x1, x2, y1, y2, z1, z2), and each item by its three dimensions (x_len, y_len, z_len). The critical piece is the
            <code class="inline-code">action_mask</code>: a boolean matrix where entry (e, i) is True if item i can legally be placed in EMS e.
        </p>

        <p>
            How does the environment compute this joint action mask? For each EMS and each item, it tests whether the item fits in that EMS and whether the item is still available. This is implemented with nested <code class="inline-code">jax.vmap</code>,
            so the entire mask computation happens in parallel:
        </p>

        <pre><code><span class="comment"># From jumanji/environments/packing/bin_pack/env.py</span>
<span class="keyword">def</span> <span class="function">is_action_allowed</span>(ems, ems_mask, item, item_mask, item_placed):
    item_fits_in_ems = item_fits_in_item(item, item_from_space(ems))
    <span class="keyword">return</span> ~item_placed & item_mask & ems_mask & item_fits_in_ems

action_mask = jax.vmap(
    jax.vmap(is_action_allowed, in_axes=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)),
    in_axes=(<span class="number">0</span>, <span class="number">0</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>),
)(obs_ems, obs_ems_mask, items, items_mask, items_placed)</code></pre>

        <p>
            One subtle detail: the full environment can track more than 40 EMS internally, but the observation only returns the <code class="inline-code">obs_num_ems</code> largest by volume. This keeps the observation size fixed, turning a variable-structure
            geometric process into a fixed-shape tensor problem—exactly what we need for efficient JAX/XLA compilation. The environment is also strict about invalid actions: if you choose an action where the item doesn't fit or has already been placed,
            the episode terminates immediately. This makes action masking a first-class concern, both in PPO and especially in MCTS where we want zero probability mass on invalid moves.
        </p>

        <p>
            The EMS representation isn't unique to Jumanji—it's a classic way to represent free space as a non-disjoint set of maximal empty cuboids. If you're interested in the "pre-RL" perspective on this representation, Parreño et al.'s <a href="https://www.uv.es/sestio/TechRep/tr03-07.pdf"
                target="_blank">technical report</a> on GRASP heuristics and Zhao et al.'s <a href="https://eprints.soton.ac.uk/364226/1/A_20Comparative_20Review_20of_203D_20Container_20Loading_20Algorithms.pdf" target="_blank">comparative review</a> of 3D
            container loading provide excellent background. The key insight is that EMS makes the problem look like "choose a space + choose an item", which is exactly how our policy and value networks will model it.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Expert Iteration -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="exit">Expert Iteration: Teaching a Network with Search</h2>

        <p>
            When AlphaZero achieved superhuman performance at Go and Chess, the key innovation wasn't just MCTS or neural networks—it was how the two components reinforced each other. The network provides fast intuition to guide search, and search provides high-quality
            targets to improve the network. But there's an important detail that's often glossed over: AlphaZero uses <em>self-play</em> because Go and Chess are two-player games. The network plays against itself, generating training data
            from both sides of the board.
        </p>

        <p>
            Bin packing is fundamentally different. There's no opponent—it's a single-agent planning task. So we can't do self-play in the traditional sense. Instead, we use what Anthony et al. called <a href="https://arxiv.org/abs/1705.08439" target="_blank">"Expert Iteration"</a>:
            we treat MCTS as an expert teacher that generates improved policy targets, then distill that knowledge into a neural network. The network gets better, which makes MCTS stronger (since it uses the network for value estimates and prior probabilities),
            which produces even better targets, and so on.
        </p>

        <p>
            The core loop looks like this:
        </p>

        <pre>
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│   ┌─────────┐      improved       ┌─────────┐      distill      ┌───────┐   │
│   │  MCTS   │ ───────────────────►│ Policy  │ ─────────────────►│  NN   │   │
│   │ Search  │      targets        │ Targets │      into         │Policy │   │
│   └─────────┘                     └─────────┘                   └───────┘   │
│        ▲                                                            │       │
│        │                          next iteration                    │       │
│        └────────────────────────────────────────────────────────────┘       │
│                                                                             │
│   The network improves → MCTS becomes stronger → better targets → ...       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
        </pre>

        <p>
            This is essentially approximate policy iteration with a very strong policy improvement operator. In classical policy iteration, you evaluate your current policy and then greedily improve it. Here, MCTS does both at once—it uses the network's value estimates
            to evaluate states deep in the search tree, and it uses the network's policy to guide which branches to explore. The resulting action distribution from MCTS is provably better than the raw network output (under certain conditions), so training
            the network to match it constitutes genuine improvement.
        </p>

        <p>
            A subtle but important detail from the Expert Iteration paper: when distilling search into a network, it's better to train on the <em>distribution</em> over actions from search rather than only the argmax. This is sometimes called "Tree-Policy
            Targets" and it's cost-sensitive—when MCTS is uncertain between two actions, the network isn't punished as harshly for choosing either. The loss function is simply cross-entropy:
        </p>

        <div class="math-block">
            \[ \mathcal{L}_{\text{policy}}(\theta) \;=\; -\sum_{a} \pi_{\text{MCTS}}(a\mid s)\,\log \pi_\theta(a\mid s) \]
        </div>

        <p>
            This is one reason Expert Iteration often feels more stable than policy gradient methods: the supervision signal is dense over actions, not a single noisy sample. Combined with the fact that we have perfect environment dynamics for search, Expert Iteration
            becomes remarkably effective for combinatorial optimization problems like bin packing.
        </p>

        <h3>How Expert Iteration Compares to PPO</h3>

        <p>
            It's worth understanding exactly why Expert Iteration outperforms PPO on this task. PPO is an on-policy policy gradient method that maximizes a clipped surrogate objective. The canonical form is:
        </p>

        <div class="math-block">
            \[ L^{\text{CLIP}}(\theta)=\mathbb{E}_t\left[\min\left(r_t(\theta)\,\hat{A}_t,\; \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\,\hat{A}_t\right)\right] \]
        </div>

        <p>
            where \(r_t(\theta) = \pi_\theta(a_t|s_t) / \pi_{\theta_{\text{old}}}(a_t|s_t)\) is the probability ratio. PPO works well when you can collect lots of trajectories and the environment is too complex for planning. But PPO's gradient signal is based on
            <em>sampled actions</em> and <em>sampled returns</em>, so variance control (GAE, baselines, clipping) becomes essential.
        </p>

        <p>
            Expert Iteration sidesteps this variance problem entirely. Instead of learning from a single sampled action and its noisy return, we learn from the full MCTS action distribution—a much richer signal. And because we have exact environment dynamics, MCTS
            can do genuine lookahead planning, discovering good action sequences that pure policy gradients might take much longer to find.
        </p>

        <p>
            The practical difference is stark: Expert Iteration achieves 96% volume utilization on BinPack-v2, while PPO plateaus around 90%. That 6 percentage point gap represents a meaningful improvement in packing efficiency.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: MCTS with mctx -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="mcts">Monte Carlo Tree Search with mctx</h2>

        <p>
            We use <a href="https://github.com/google-deepmind/mctx" target="_blank">mctx</a>, DeepMind's JAX-native MCTS library. The library is worth studying because it implements batched MCTS that compiles entirely under XLA—the entire search runs
            as a single compiled program, not Python loops calling into JAX. This is what makes it fast enough to compete with model-free methods on wall-clock time.
        </p>

        <p>
            The mctx library requires you to provide a "recurrent function" that simulates environment transitions. For bin packing, this is trivial because we have the exact environment dynamics—we just call <code class="inline-code">env.step</code>:
        </p>

        <pre><code><span class="keyword">import</span> mctx

<span class="keyword">def</span> <span class="function">recurrent_fn</span>(model, rng_key, action, state):
    <span class="string">"""Environment model for MCTS simulation."""</span>
    model_params, model_state = model
    
    <span class="comment"># Step the environment (we have perfect dynamics!)</span>
    action_pair = unflatten_action(action)
    next_state, timestep = jax.vmap(env.step)(state, action_pair)
    
    <span class="comment"># Get network predictions for the next state</span>
    observation = timestep.observation
    (logits, value), _ = forward.apply(
        model_params, model_state, observation, is_eval=<span class="keyword">True</span>
    )
    
    <span class="comment"># Mask invalid actions</span>
    valid_mask = get_valid_action_mask(observation.action_mask)
    logits = apply_action_mask(logits, valid_mask)
    
    <span class="keyword">return</span> mctx.RecurrentFnOutput(
        reward=timestep.reward,
        discount=timestep.discount,
        prior_logits=logits,
        value=value,
    ), next_state</code></pre>

        <p>
            With this function defined, running MCTS is straightforward. We use <code class="inline-code">gumbel_muzero_policy</code>, which provides a more principled approach to exploration than vanilla MCTS (more on this shortly):
        </p>

        <pre><code><span class="comment"># Run MCTS from the current state</span>
policy_output = mctx.gumbel_muzero_policy(
    params=model,
    rng_key=key,
    root=mctx.RootFnOutput(
        prior_logits=network_logits,
        value=network_value,
        embedding=current_state,  <span class="comment"># The state IS the embedding</span>
    ),
    recurrent_fn=recurrent_fn,
    num_simulations=<span class="number">32</span>,          <span class="comment"># Search budget per decision</span>
    invalid_actions=~valid_mask,
)

<span class="comment"># Extract the improved policy (this is our training target)</span>
mcts_policy = policy_output.action_weights  <span class="comment"># Shape: (batch, num_actions)</span></code></pre>

        <p>
            The <code class="inline-code">action_weights</code> output is the probability distribution over actions that we use as our training target. This is computed from the search tree—in Gumbel MuZero specifically, it's a softmax over the prior
            logits plus the completed Q-values, which provides a principled policy improvement guarantee.
        </p>

        <h3>Under the Hood: How mctx Implements Batched Search</h3>

        <p>
            Understanding how mctx works is instructive if you've ever tried to write MCTS and gotten stuck with Python control flow. The core insight is that the entire search loop is implemented using <code class="inline-code">jax.lax.fori_loop</code>            and <code class="inline-code">jax.lax.while_loop</code>, which compile to XLA control flow rather than Python loops. Here's the main search loop:
        </p>

        <pre><code><span class="comment"># From mctx/_src/search.py</span>
<span class="keyword">def</span> <span class="function">body_fun</span>(sim, loop_state):
    rng_key, tree = loop_state
    rng_key, simulate_key, expand_key = jax.random.split(rng_key, <span class="number">3</span>)

    <span class="comment"># Simulate: walk down the tree selecting actions</span>
    parent_index, action = simulate(
        simulate_keys, tree, action_selection_fn, max_depth)

    <span class="comment"># Check if we've reached an unexpanded node</span>
    next_node_index = tree.children_index[batch_range, parent_index, action]
    next_node_index = jnp.where(next_node_index == Tree.UNVISITED,
                                sim + <span class="number">1</span>, next_node_index)

    <span class="comment"># Expand: add new node to the tree</span>
    tree = expand(params, expand_key, tree, recurrent_fn, 
                  parent_index, action, next_node_index)
    
    <span class="comment"># Backup: propagate values up the tree</span>
    tree = backward(tree, next_node_index)
    <span class="keyword">return</span> rng_key, tree</code></pre>

        <p>
            The simulation phase uses a while loop to walk down the existing tree until it finds an unvisited edge. At expansion, mctx calls your recurrent function to get the value and prior for the new node. The backup phase then propagates the leaf value up to
            the root using an incremental mean update:
        </p>

        <div class="math-block">
            \[ Q_{\text{parent}} \leftarrow \frac{N_{\text{parent}} \cdot Q_{\text{parent}} + G}{N_{\text{parent}} + 1} \quad\text{where}\quad G = r + \gamma \cdot V_{\text{leaf}} \]
        </div>

        <p>
            Because all of this is expressed as JAX control flow, the entire search compiles to a single XLA program. Combined with <code class="inline-code">vmap</code> over the batch dimension, this makes it feasible to run dozens of simulations per
            decision at scale.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Gumbel Exploration -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="gumbel">Gumbel Exploration</h2>

        <p>
            We use Gumbel MuZero rather than vanilla MCTS for a specific reason: when you have many actions and only a small simulation budget, traditional AlphaZero-style exploration can fail to visit enough actions to guarantee improvement. Gumbel MuZero, introduced
            in <a href="https://openreview.net/forum?id=bERaNdoegnO" target="_blank">"Policy Improvement by Planning with Gumbel"</a> (ICLR 2022), fixes this by sampling actions without replacement in a principled way.
        </p>

        <p>
            The foundation is the Gumbel-Max trick for sampling from a categorical distribution. If your policy network outputs logits \(\ell(a)\), you can sample from \(\pi(a) = \text{softmax}(\ell)\) by adding Gumbel noise and taking an argmax:
        </p>

        <div class="math-block">
            \[ g(a) \sim \text{Gumbel}(0,1), \qquad A = \arg\max_a \left[g(a) + \ell(a)\right] \]
        </div>

        <p>
            The paper extends this to the <strong>Gumbel-Top-k</strong> trick, which samples k actions without replacement. At the root node, Gumbel MuZero uses this to select a subset of candidate actions, then allocates the simulation budget across
            those actions using <strong>Sequential Halving</strong>—a bandit algorithm optimized for simple regret rather than cumulative regret. This is the right objective when you only care about finding the best action, not about the path you took
            to find it.
        </p>

        <p>
            You can see the mechanics directly in the mctx code:
        </p>

        <pre><code><span class="comment"># From mctx/_src/policies.py (gumbel_muzero_policy)</span>

<span class="comment"># 1) Mask invalid actions at the root</span>
root = root.replace(
    prior_logits=_mask_invalid_actions(root.prior_logits, invalid_actions)
)

<span class="comment"># 2) Sample Gumbel noise (same shape as logits)</span>
rng_key, gumbel_rng = jax.random.split(rng_key)
gumbel = gumbel_scale * jax.random.gumbel(
    gumbel_rng, shape=root.prior_logits.shape, dtype=root.prior_logits.dtype
)

<span class="comment"># 3) Run batched tree search with Gumbel-aware action selection</span>
search_tree = search.search(
    params=params,
    rng_key=rng_key,
    root=root,
    recurrent_fn=recurrent_fn,
    root_action_selection_fn=functools.partial(
        action_selection.gumbel_muzero_root_action_selection,
        num_simulations=num_simulations,
        max_num_considered_actions=max_num_considered_actions,
        qtransform=qtransform,
    ),
    interior_action_selection_fn=functools.partial(
        action_selection.gumbel_muzero_interior_action_selection,
        qtransform=qtransform,
    ),
    num_simulations=num_simulations,
    invalid_actions=invalid_actions,
    extra_data=action_selection.GumbelMuZeroExtraData(root_gumbel=gumbel),
)</code></pre>

        <p>
            One important detail that might surprise you: in Gumbel MuZero, the returned <code class="inline-code">action_weights</code> are <em>not</em> computed from visit counts like in AlphaZero. Instead, they're <code class="inline-code">softmax(prior_logits + completed_qvalues)</code>—a
            direct implementation of the policy improvement operator from the paper. This theoretical grounding is part of why Gumbel MuZero works well with small simulation budgets.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: JAX Parallelism -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="jax">JAX Parallelism</h2>

        <p>
            The historical challenge with Expert Iteration is computational cost. Running 32 MCTS simulations per action adds significant overhead compared to a single network forward pass. But JAX's parallelization primitives change the calculus dramatically.
        </p>

        <p>
            The key primitives are <code class="inline-code">jax.vmap</code> for vectorizing over the batch dimension and <code class="inline-code">jax.pmap</code> for distributing across devices. With <code class="inline-code">vmap</code>, a single line
            of code turns a function that processes one environment into a function that processes thousands in parallel:
        </p>

        <pre><code><span class="comment"># Without vmap: process episodes one at a time</span>
<span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):
    state, timestep = env.step(states[i], actions[i])

<span class="comment"># With vmap: all episodes processed in parallel</span>
state, timestep = jax.vmap(env.step)(states, actions)</code></pre>

        <p>
            The <code class="inline-code">pmap</code> primitive replicates computation across all available GPUs or TPUs. Each device processes a shard of the batch independently, and gradients are synchronized using <code class="inline-code">jax.lax.pmean</code>:
        </p>

        <pre><code><span class="decorator">@partial</span>(jax.pmap, axis_name=<span class="string">"devices"</span>)
<span class="keyword">def</span> <span class="function">train_step</span>(model, opt_state, batch):
    <span class="comment"># Compute gradients on this device's shard</span>
    grads = jax.grad(loss_fn)(model, batch)
    
    <span class="comment"># Synchronize gradients across all devices</span>
    grads = jax.lax.pmean(grads, axis_name=<span class="string">"devices"</span>)
    
    <span class="comment"># Apply optimizer update</span>
    updates, opt_state = optimizer.update(grads, opt_state)
    model = optax.apply_updates(model, updates)
    <span class="keyword">return</span> model, opt_state</code></pre>

        <p>
            Together, vmap and pmap give us massive throughput. On 4 GPUs with batch size 1024, we process roughly 20,000 MCTS-guided decisions per second—competitive with PPO's sample efficiency despite running 32 simulations per decision. The JAX documentation
            on <a href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html" target="_blank">parallelism</a> provides an excellent introduction to these concepts.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Neural Network Architecture -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="architecture">Neural Network Architecture</h2>

        <p>
            We adapt Jumanji's <a href="https://github.com/instadeepai/jumanji/blob/main/jumanji/training/networks/bin_pack/actor_critic.py" target="_blank">A2C architecture</a> for our policy-value network. The key insight is using <strong>cross-attention</strong>            between EMS tokens and item tokens. This lets the network reason about which items fit in which spaces, with the action mask gating the attention to only consider valid placement combinations.
        </p>

        <p>
            The architecture processes EMS and item tokens through alternating layers of self-attention (within each token type) and cross-attention (between types). After encoding, the policy head computes logits via a bilinear form—<code class="inline-code">logits[e,i] = ems_h[e] · items_h[i]</code>—which
            naturally produces the (E × I) shaped output we need. The value head pools the embeddings and predicts expected utilization in [0, 1].
        </p>

        <pre><code><span class="comment"># Architecture overview</span>
<span class="comment">#</span>
<span class="comment">#   ┌─────────────┐     ┌─────────────┐</span>
<span class="comment">#   │  EMS tokens │     │ Item tokens │</span>
<span class="comment">#   │ (40 × 6D)   │     │ (20 × 3D)   │</span>
<span class="comment">#   └──────┬──────┘     └──────┬──────┘</span>
<span class="comment">#          │                   │</span>
<span class="comment">#          ▼                   ▼</span>
<span class="comment">#   ┌─────────────┐     ┌─────────────┐</span>
<span class="comment">#   │ Self-Attn   │     │ Self-Attn   │</span>
<span class="comment">#   └──────┬──────┘     └──────┬──────┘</span>
<span class="comment">#          │                   │</span>
<span class="comment">#          └────────┬──────────┘</span>
<span class="comment">#                   ▼</span>
<span class="comment">#          ┌───────────────┐</span>
<span class="comment">#          │ Cross-Attn    │  ← EMS ↔ Items interaction</span>
<span class="comment">#          │ (bidirectional)│    (gated by action_mask)</span>
<span class="comment">#          └───────┬───────┘</span>
<span class="comment">#                  │</span>
<span class="comment">#          ┌───────┴───────┐</span>
<span class="comment">#          ▼               ▼</span>
<span class="comment">#   ┌─────────────┐  ┌─────────────┐</span>
<span class="comment">#   │ Policy Head │  │ Value Head  │</span>
<span class="comment">#   │ (E×I logits)│  │ (scalar)    │</span>
<span class="comment">#   └─────────────┘  └─────────────┘</span></code></pre>

        <p>
            We use <a href="https://dm-haiku.readthedocs.io/" target="_blank">Haiku</a> to build the network. Haiku's <code class="inline-code">hk.transform_with_state</code> pattern cleanly separates the stateful module definition from the pure functional
            interface that JAX requires:
        </p>

        <pre><code><span class="keyword">import</span> haiku <span class="keyword">as</span> hk

<span class="keyword">def</span> <span class="function">forward_fn</span>(observation, is_eval=<span class="keyword">False</span>):
    net = BinPackPolicyValueNet(
        num_transformer_layers=<span class="number">4</span>,
        transformer_num_heads=<span class="number">4</span>,
        transformer_key_size=<span class="number">32</span>,
        transformer_mlp_units=(<span class="number">256</span>, <span class="number">256</span>),
    )
    <span class="keyword">return</span> net(observation, is_training=<span class="keyword">not</span> is_eval)

<span class="comment"># Transform to pure functions</span>
forward = hk.without_apply_rng(hk.transform_with_state(forward_fn))

<span class="comment"># Initialize parameters</span>
params, state = forward.init(rng_key, dummy_obs)

<span class="comment"># Apply is now a pure function</span>
(logits, value), new_state = forward.apply(params, state, observation)</code></pre>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Experiments -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="experiments">Experiments</h2>

        <p>
            We trained both Expert Iteration and PPO on BinPack-v2 for 800 iterations across 5 random seeds. To ensure a fair comparison, both methods use the same network architecture, batch sizes, and evaluation protocol. The key hyperparameters are:
        </p>

        <table>
            <tr>
                <th>Hyperparameter</th>
                <th>Expert Iteration</th>
                <th>PPO</th>
            </tr>
            <tr>
                <td>Rollout batch size</td>
                <td>1024</td>
                <td>1024</td>
            </tr>
            <tr>
                <td>Training batch size</td>
                <td>4096</td>
                <td>4096</td>
            </tr>
            <tr>
                <td>MCTS simulations</td>
                <td>32</td>
                <td>—</td>
            </tr>
            <tr>
                <td>PPO epochs per iteration</td>
                <td>—</td>
                <td>4</td>
            </tr>
            <tr>
                <td>Learning rate</td>
                <td>1e-3</td>
                <td>3e-4</td>
            </tr>
            <tr>
                <td>Transformer layers</td>
                <td>4</td>
                <td>4</td>
            </tr>
        </table>

        <figure>
            <img src="ppo_vs_alphazero.gif" alt="PPO vs Expert Iteration learning curves">
            <figcaption>
                <strong>Figure 1.</strong> Learning curves comparing Expert Iteration (light blue) vs PPO (dark blue) on BinPack-v2. Shaded regions show ±1 standard error across 5 seeds. Expert Iteration converges to 96% volume utilization while PPO plateaus
                around 90%.
            </figcaption>
        </figure>

        <p>
            The results are clear: Expert Iteration achieves 96% volume utilization compared to PPO's 90%, a 6 percentage point improvement that translates to significantly better packing efficiency in practice. Despite running 32 MCTS simulations per action, JAX
            parallelism keeps training time competitive—both methods reach convergence in roughly the same wall-clock time.
        </p>

        <p>
            The learning curves also show that Expert Iteration is more stable. PPO exhibits the characteristic variance of policy gradient methods, with performance fluctuating throughout training. Expert Iteration's learning curve is smoother, reflecting the richer
            supervision signal from MCTS action distributions.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: Implementation Details -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="implementation">Implementation Details</h2>

        <p>
            A few implementation details are worth highlighting, as they address common pitfalls when working with JAX and MCTS.
        </p>

        <h3>Using jax.lax.scan for Efficient Loops</h3>

        <p>
            Python loops inside JIT-compiled functions are unrolled at compile time, causing slow compilation and memory issues for long episodes. The solution is <code class="inline-code">jax.lax.scan</code>, which compiles the loop body once and executes
            it repeatedly:
        </p>

        <pre><code><span class="comment"># Inefficient: Python loop gets unrolled</span>
<span class="keyword">def</span> <span class="function">rollout_bad</span>(state, keys):
    trajectory = []
    <span class="keyword">for</span> key <span class="keyword">in</span> keys:
        state, data = step(state, key)
        trajectory.append(data)
    <span class="keyword">return</span> trajectory

<span class="comment"># Efficient: scan compiles once, runs fast</span>
<span class="keyword">def</span> <span class="function">rollout_good</span>(state, keys):
    <span class="keyword">def</span> <span class="function">step_fn</span>(carry, key):
        state = carry
        state, data = step(state, key)
        <span class="keyword">return</span> state, data
    
    final_state, trajectory = jax.lax.scan(step_fn, state, keys)
    <span class="keyword">return</span> trajectory</code></pre>

        <h3>Action Masking Without NaNs</h3>

        <p>
            Invalid actions must be masked before computing the softmax for action selection. A common mistake is using <code class="inline-code">-inf</code> for invalid logits, which causes NaN when all actions are invalid (as can happen at episode termination).
            The solution is to use a large but finite negative value:
        </p>

        <pre><code><span class="keyword">def</span> <span class="function">apply_action_mask</span>(logits, valid_mask):
    <span class="string">"""Set invalid action logits to a large negative value."""</span>
    <span class="comment"># Center for numerical stability</span>
    logits = logits - jnp.max(logits, axis=-<span class="number">1</span>, keepdims=<span class="keyword">True</span>)
    <span class="comment"># Use finite minimum (not -inf) to avoid NaN</span>
    <span class="keyword">return</span> jnp.where(valid_mask, logits, jnp.finfo(logits.dtype).min)</code></pre>

        <p>
            The mctx library uses the same approach for the same reason—at the end of an episode, all actions can become invalid, and the code needs to handle this gracefully.
        </p>

        <h3>When to Use Expert Iteration</h3>

        <p>
            Expert Iteration is particularly well-suited to problems where you have exact dynamics (so MCTS can simulate accurately), episodes are relatively short (so MCTS overhead doesn't compound excessively), actions have complex dependencies that gradient signals
            might miss, and you have parallel compute to amortize the cost of search. Bin packing hits all four criteria.
        </p>

        <p>
            PPO remains preferable when dynamics must be learned rather than simulated, horizons are very long (hundreds of steps), action spaces are simple with clear reward gradients, or when you need the simplicity of a model-free approach.
        </p>

        <!-- ═══════════════════════════════════════════════════════════════════ -->
        <!-- SECTION: References -->
        <!-- ═══════════════════════════════════════════════════════════════════ -->

        <h2 id="refs">References</h2>

        <h3>Code</h3>

        <p>
            The complete implementation is available on <a href="https://github.com/Aneeshers/expert-iteration-rl" target="_blank">GitHub</a>. Key dependencies include <a href="https://github.com/google-deepmind/mctx" target="_blank">mctx</a> for MCTS,
            <a href="https://github.com/instadeepai/jumanji" target="_blank">Jumanji</a> for the BinPack environment, <a href="https://dm-haiku.readthedocs.io/" target="_blank">Haiku</a> for neural networks, and <a href="https://optax.readthedocs.io/"
                target="_blank">Optax</a> for optimization. The <a href="https://github.com/sotetsuk/pgx/tree/main/examples/alphazero" target="_blank">PGX AlphaZero</a> example was a helpful reference for JAX-native AlphaZero patterns.
        </p>

        <h3>Papers</h3>

        <ol class="references">
            <li>
                Anthony et al., <a href="https://arxiv.org/abs/1705.08439" target="_blank">"Thinking Fast and Slow with Deep Learning and Tree Search"</a>, NeurIPS 2017. The original Expert Iteration paper.
            </li>
            <li>
                Danihelka et al., <a href="https://openreview.net/forum?id=bERaNdoegnO" target="_blank">"Policy Improvement by Planning with Gumbel"</a>, ICLR 2022. The theoretical foundation for Gumbel MuZero.
            </li>
            <li>
                Grill et al., <a href="https://arxiv.org/abs/2007.12509" target="_blank">"Monte-Carlo Tree Search as Regularized Policy Optimization"</a>, ICML 2020. Interprets MCTS as approximately solving a regularized policy optimization problem.
            </li>
            <li>
                Silver et al., <a href="https://www.nature.com/articles/nature24270" target="_blank">"Mastering the game of Go without human knowledge"</a>, Nature 2017. The AlphaZero paper.
            </li>
            <li>
                Schrittwieser et al., <a href="https://arxiv.org/abs/1911.08265" target="_blank">"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"</a>, Nature 2020. The MuZero paper.
            </li>
            <li>
                Schulman et al., <a href="https://arxiv.org/abs/1707.06347" target="_blank">"Proximal Policy Optimization Algorithms"</a>, arXiv 2017. The PPO paper.
            </li>
            <li>
                Bonnet et al., <a href="https://arxiv.org/abs/2306.09884" target="_blank">"Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX"</a>, 2023. The Jumanji environment suite including BinPack.
            </li>
            <li>
                Karnin, Koren, Somekh, <a href="https://proceedings.mlr.press/v28/karnin13.pdf" target="_blank">"Almost Optimal Exploration in Multi-Armed Bandits"</a>, ICML 2013. The Sequential Halving algorithm used in Gumbel MuZero.
            </li>
        </ol>

        <h2>Citation</h2>

        <pre><code>@misc{muppidi2026expertiteration,
  title={Expert Iteration for 3D Bin Packing},
  author={Muppidi, Aneesh},
  year={2026},
  url={https://github.com/Aneeshers/expert-iteration-rl}
}</code></pre>

        <footer>
            <p>
                <a href="https://aneeshers.github.io"><strong>Aneesh Muppidi</strong></a><br> February 2026
            </p>
            <p style="margin-top: 16px;">
                <a href="https://github.com/Aneeshers/expert-iteration-rl">GitHub</a> ·
                <a href="https://twitter.com/aneeshers">Twitter</a>
            </p>
        </footer>
    </div>
</body>

</html>